{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- ARTS multi-FOV 2DVAR surface retrievals --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  David Duncan, Chalmers University, Mar 2019\n",
    "\n",
    "# this particular version (real1) attempts retrieval from real AMSR2 L1R data\n",
    "#  over a very limited spatial domain doing 2dvar retrieval of sst/wind\n",
    "\n",
    "\n",
    "#  this script uses ARTS OEM to experiment with retrieval of a synthetic scene. \n",
    "#    the goal here is to probe the spatial resolution of geophysical parameter \n",
    "#    that may be retrievable with such a sensor, focused on AMSR2 and surface params\n",
    "\n",
    "# this setup, in steps/words:\n",
    "### - define an antenna response that is frequency dependent, gaussian, symmetric\n",
    "### - define sensor position and line of sight for each scan at middle of scan\n",
    "### - define an angular grid (zenith & azimuth) for each scan that encompasses npix\n",
    "### - sensor angles are all 'absolute' whereas the angular grid and antenna response \n",
    "###    are all relative to the central bore sight of the scan\n",
    "### - a lat/lon grid is also defined that must encompass all observation points simulated\n",
    "### - the same angular grid and simulation setup is assumed/copied for all nscans, and \n",
    "###    mblock_dlos_grid allows all simulations to be run at the same time\n",
    "\n",
    "### *** this setup outputs simulates TBs for chosen channels of AMSR2\n",
    "###    with antenna patterns taken into account and decoupled from any retrieval grid, with\n",
    "###    pencil beam calculations sampling the antenna pattern according to the angular grid\n",
    "###    defined\n",
    "\n",
    "### a priori covariances are defined in terms of standard deviations and decorrelation lengths\n",
    "###  and observation error covariances are frequency dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cmocean import cm as cmo\n",
    "from h5py import File\n",
    "from netCDF4 import Dataset\n",
    "import time\n",
    "startt = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define # of vars, channels, size of retrieval, and covariance assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequency subset, defined by AMSR2 frequency indices: 0=6.9GHz, 1=7.3, 2=10.6, 3=18.7, 4=23.8, 5=36.5, 6=89\n",
    "#fsub  = np.array([0, 1, 2, 3, 5]) # choose which frequencies to run (runs both polarisations--v/h)\n",
    "fsub = np.array([0,1,2,3,4,5,6])\n",
    "sfsub= str(\"frs\")+\"\".join(map(str,fsub)) #  string of frequencies (for plot outputs)\n",
    "\n",
    "deco_sst = 1.0  # SST decorrelation length of a priori [deg] -- set to near-zero if wanting none\n",
    "deco_wsp = 1.0  # wind decorrelation length of a priori [deg] \n",
    "pv = 'v1h_ap20-1d_r10-6a_'+sfsub # plot version (end of plots' filenames)\n",
    "sv = True      # whether or not to save output plots\n",
    "\n",
    "nrvar    =   2   # number of retrieval variables, 1 will be SST only, 2 for SST+windspeed\n",
    "\n",
    "#npix     =  23   # pixels across each scan considered\n",
    "#nscans   =  13   # consecutive scans considered\n",
    "angfac   =   6   # angular samples per degree or between pix -- should be min ~10 (but depends on freqs used)\n",
    "sx_sst   = 2.0   # a priori std deviation for SST\n",
    "sx_wsp   = 2.0   # a priori std deviation for wind speed (if nrvar > 1)\n",
    "#xcorr    = 0.0   # cross correlation between SST and wind speed -- not currently used (keep sparse matrix for speed)\n",
    "\n",
    "# these should likely remain fixed:\n",
    "noise_fac = 1.0  # multiply sense noise (nedt) by this factor\n",
    "nedt = np.array([0.34, 0.43, 0.7, 0.7, 0.6, 0.7, 1.2])[fsub] # from published NEdT values\n",
    "npol     =  2    # number of polarizations considered (2=V/H both, 1=intensity only)\n",
    "\n",
    "#  note: for speed, the angular grid is the biggest limiting factor, so if using low frequencies only\n",
    "#    then angfac<~5 is quite justified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define lat/lon grid that observation points and angular grid lie within\n",
    "lat0     = 61  # center of grid in lat, lon \n",
    "lon0     = -54.0\n",
    "nlat, nlon =  50, 50         # number of lat and lon divisions\n",
    "latwid, lonwid = 5, 5    # actual grid widths are double these\n",
    "\n",
    "# retrieval grid resolution (set down below)\n",
    "r_resa, r_reso = 0.1, 0.1\n",
    "\n",
    "resa = 2*latwid/nlat  #lat grid resolution\n",
    "reso = 2*lonwid/nlon  #lon grid resolution\n",
    "print('Grid resolution is ', resa, reso)\n",
    "print('Retr grid resolution is ', r_resa, r_reso)\n",
    " \n",
    "la_grid = lat0 + np.arange(-latwid, latwid+resa, resa)\n",
    "lo_grid = lon0 + np.arange(-lonwid, lonwid+reso, reso)\n",
    "# keep these (non-ARTS vars) for sake of plotting at the end!\n",
    " \n",
    "#print('corners of la_grid: ',la_grid)\n",
    "#print('midpoints of la_grid (ARTS ws grid): ',latgrid[0:nlat]+np.diff(la_grid)*.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in AMSR2 L1R data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in L1R file and calculate spacecraft position in terms of geocentric lat/lon\n",
    "afile = 'GW1AM2_201609211515_089A_L1SGRTBR_2220220.h5' # in local dir\n",
    "dorb = afile[-35:-20]\n",
    "amf = File(afile)\n",
    "\n",
    "altitude = 1e3* float(bytes.decode(amf.attrs['SatelliteAltitude'][0])[:-2]) # assumed constant for all scans\n",
    "print(altitude,'m')\n",
    "lof  = amf['Land_Ocean Flag 6 to 36'][0,:,:].transpose()[:,30:-30] # just 6GHz FOV land %age\n",
    "navd = amf['Navigation Data'][:] # [scans,6] first 3 are X,Y,Z in WGS84 positioning, units m, m/s\n",
    "#eia  = amf['Earth Incidence'][:].transpose()*.01 # earth incidence angle (~55deg)  -- NOT CURRENTLY USED\n",
    "#eaz  = amf['Earth Azimuth'][:].transpose()*.01 # earth azimuth (deg from N)\n",
    "#eaz += 180\n",
    "#eaz[eaz>180] -= 360  # convert to ARTS earth azimuth convention, make sure it's between -180,180\n",
    "\n",
    "csub = np.sort(np.append(fsub*2,fsub*2+1))  # channels subset (assumes V/H always run together)\n",
    "ch_str = np.array(['6V','6H','7V','7H','10V','10H','18V','18H',\n",
    "                   '23V','23H','36V','36H','89V','89H'])[csub] \n",
    "\n",
    "# choose channels and convolution resolution (these are native except 89!)\n",
    "tblist = np.array(['Brightness Temperature (res06,6.9GHz,V)','Brightness Temperature (res06,6.9GHz,H)',\\\n",
    "          'Brightness Temperature (res06,7.3GHz,V)','Brightness Temperature (res06,7.3GHz,H)',\\\n",
    "          'Brightness Temperature (res10,10.7GHz,V)','Brightness Temperature (res10,10.7GHz,H)',\\\n",
    "          'Brightness Temperature (res23,18.7GHz,V)','Brightness Temperature (res23,18.7GHz,H)',\\\n",
    "          'Brightness Temperature (res23,23.8GHz,V)','Brightness Temperature (res23,23.8GHz,H)',\\\n",
    "          'Brightness Temperature (res36,36.5GHz,V)','Brightness Temperature (res36,36.5GHz,H)',\\\n",
    "          'Brightness Temperature (res36,89.0GHz,V)','Brightness Temperature (res36,89.0GHz,H)' ])[csub]\n",
    "nch = len(tblist)\n",
    "ch_str = np.array(['6V','6H','7V','7H','10V','10H','18V','18H','23V','23H','36V','36H','89V','89H'])[csub]\n",
    "print('Channels used in retrieval: ',ch_str, nch)\n",
    "apix = 243  # pixels across one AMSR2 scan\n",
    "ascans = np.shape(amf[tblist[0]][:])[0] \n",
    "print(ascans)\n",
    "tbs = np.zeros([apix,ascans,nch])\n",
    "for c,chstr in enumerate(tblist):\n",
    "    tbs[:,:,c] = .01 * amf[chstr][:,:].transpose()  # tbs saved with factor 100\n",
    "\n",
    "# NOTE: there are 30 'overlap' scans included on either end of JAXA files, so shrink array to match L1C:\n",
    "ascans -= 30*2\n",
    "tbs = tbs[:, 30:-30, :].reshape(ascans*apix,nch)\n",
    "#print(shinfo(tbs),ascans)  \n",
    "\n",
    "alo = amf['Longitude of Observation Point for 89A'][:].transpose()[::2,30:-30] # every other one has a low-freq\n",
    "ala = amf['Latitude of Observation Point for 89A'][:].transpose()[::2,30:-30]  # obs point (half the sampling)\n",
    "\n",
    "from astropy.coordinates import EarthLocation\n",
    "# sensor_losGeometricFromSensorPosToOtherPositions will draw the line between two distinct points \\n\",\n",
    "#   such as SCposition and position on the ground\n",
    "sloc = EarthLocation.from_geocentric(navd[30:-30,0],navd[30:-30,1],navd[30:-30,2], unit='m')\n",
    "SClat, SClon = sloc.lat.value, sloc.lon.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply calibration offsets to L1R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration offsets (L1 Tb + offset... so negative offsets indicate L1 is too high)\n",
    "\n",
    "# from xcal table file, following same conventions: \n",
    "# \"The inter-calibration table consists of a series of tie points for each channel. The table contains two lines\n",
    "# of information for each channel. The first column contains the channel number from 1 to N and the second\n",
    "# column contains the number of tie points for that channel. The remaining values in the first line consist of\n",
    "# the temperature values in Kelvin for each tie point and the second line contains the calibration offset\n",
    "# values in Kelvin for each tie point. Note that the resulting offset values are added to added to the Tb.\"\n",
    "#### the values are just in channel order, from 6V to 89H  (left is V, right H)\n",
    "####   and to keep separate the GPM and empirical offsets, GPM/1DVAR is first for each (6,7, and 23H DD-only)\n",
    "ch_off_c = [-0.42 -0.04, -2.15 -0.44,\\\n",
    "             0.00 -0.63,  0.00 -3.12,\\\n",
    "            -4.23 +0.74, -2.99 -1.09,\\\n",
    "            -5.30, -2.74,\\\n",
    "            -4.69, -3.70,\\\n",
    "            -2.51, -4.55,\\\n",
    "            -0.76, -2.29,\\\n",
    "            -1.14, -1.79] \n",
    "# for low freqs, numbers are just copied from cold cal point\n",
    "ch_off_h = [-0.83, -3.36,\\\n",
    "            -1.00, -3.91,\\\n",
    "            -0.96, -0.80,\\\n",
    "            -0.45, -0.09,\\\n",
    "            -1.54, -3.70,\\\n",
    "            -0.29, -0.31,\\\n",
    "            -0.02, -0.11,\\\n",
    "            -0.20, -0.22]\n",
    "ch_off = np.stack([ch_off_c,ch_off_h]).transpose() # into one array for ease\n",
    "# 'tie points' are in K, corresponding to cold and warm calibration offset scene values\n",
    "#  [these will differ from GPM cal table for low freqs]\n",
    "tie_points = np.asarray([[172,290], [83,290], [172,290], [83,290], [176,290], [88,288], \\\n",
    "                         [174,290], [99,289], [188,291], [87,90],  \\\n",
    "                         [201,288], [130,287],[235,289], [177,289],[235,289],[177,289]])\n",
    "# tbs modified by these calibration offsets involve a linear interpolation between tie points\n",
    "#  and constant value outside known values:\n",
    "#np.save('data/caltable_180119_fastem-1dvar_AMSR2_tiepoints.npy',tie_points) # nch x 2 (cold and warm cal points)\n",
    "#np.save('data/caltable_180119_fastem-1dvar_AMSR2_offsets.npy',ch_off) # nch x 2 (cold and warm cal)\n",
    "tb_cal  = tbs #np.zeros_like(tbs)\n",
    "for c in range(nch):\n",
    "    lodex = np.where( tbs[:,c] <= tie_points[c,0] )[0]\n",
    "    tb_cal[lodex,c] = tbs[lodex,c] + ch_off[c,0]\n",
    "    hidex = np.where( tbs[:,c] >= tie_points[c,1] )[0]\n",
    "    tb_cal[hidex,c] = tbs[hidex,c] + ch_off[c,1]\n",
    "    \n",
    "    midex = np.logical_and( tbs[:,c] > tie_points[c,0],  tbs[:,c] < tie_points[c,1] )\n",
    "    tb_cal[midex,c] = tbs[midex,c] + ( ((ch_off[c,1]-ch_off[c,0])/(tie_points[c,1]-tie_points[c,0])) \\\n",
    "                                         *(tbs[midex,c]-tie_points[c,0]) + ch_off[c,0])\n",
    "    \n",
    "print(shinfo(tb_cal))\n",
    "tb_cal = tb_cal.reshape(apix,ascans,nch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxt, pxp = 122, 144    # pixel # start, stop\n",
    "sct, scp = 1670, 1689   # scan # start stop\n",
    "\n",
    "if np.max(lof[pxt:pxp,sct:scp]) > 0:\n",
    "    print('LAND CONTAMINATION WITHIN AT LEAST ONE FOV! STOPPING!')\n",
    "    ANDSTOP\n",
    "pxm, scm = int(np.mean([pxt,pxp])), int(np.mean([sct,scp]))  # middle pix, scan of range\n",
    "npix = pxp-pxt\n",
    "nscans = scp-sct\n",
    "am_range = [np.min(ala[pxt:pxp,sct:scp]), np.max(ala[pxt:pxp,sct:scp]), \n",
    "            np.min(alo[pxt:pxp,sct:scp]), np.max(alo[pxt:pxp,sct:scp])]\n",
    "print('AMSR2 pixel lat range: ', am_range[0:2])\n",
    "print('AMSR2 pixel lon range: ', am_range[2:])\n",
    "#gx,gy = np.meshgrid(ala[pxt:pxp,sct:scp], alo[pxt:pxp,sct:scp])\n",
    "#gcartopts(tbs[pxt:pxp,sct:scp,0], ala[pxt:pxp,sct:scp], alo[pxt:pxp,sct:scp], 1.0, 100,200, \n",
    "#gcartopts(tbs[pxt:pxp,sct:scp,0], gx, gy, 1.0, 100,200, \n",
    "#          region=[la_grid.min(),la_grid.max(), lo_grid.min(),lo_grid.max()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up ARTS environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### -n1\n",
    "%env ARTS_INCLUDE_PATH=/home/dudavid/arts/controlfiles/\n",
    "%env ARTS_BUILD_PATH=/home/dudavid/arts/build/\n",
    "%env ARTS_DATA_PATH=/home/dudavid/arts/arts-xml/ \n",
    "#%env OMP_NUM_THREADS=1\n",
    "# to limit computation to single core or set # of cores, set the above ARTS API keyword\n",
    "\n",
    "%matplotlib inline\n",
    "#from h5py import File\n",
    "from scipy.spatial.distance import pdist      # calculate pair-wise distances (euclidean)\n",
    "from scipy.spatial.distance import squareform # convert from reduced matrix to square\n",
    "\n",
    "from typhon.arts.workspace import Workspace, arts_agenda\n",
    "ws = Workspace(verbosity=0)\n",
    "ws.execute_controlfile(\"general/general.arts\")\n",
    "ws.execute_controlfile(\"general/continua.arts\")\n",
    "ws.execute_controlfile(\"general/agendas.arts\")\n",
    "ws.execute_controlfile(\"general/planet_earth.arts\")\n",
    "ws.refellipsoidEarth(model=\"WGS84\") ## set ellipsoid to WGS84\n",
    "\n",
    "from typhon.arts.workspace.variables import *\n",
    "\n",
    "# set various ARTS agendas:\n",
    "ws.Copy( ws.abs_xsec_agenda, ws.abs_xsec_agenda__noCIA )\n",
    "ws.Copy( ws.iy_main_agenda, ws.iy_main_agenda__Emission )\n",
    "ws.Copy( ws.iy_space_agenda, ws.iy_space_agenda__CosmicBackground )\n",
    "ws.Copy( ws.propmat_clearsky_agenda, ws.propmat_clearsky_agenda__OnTheFly )\n",
    "ws.Copy( ws.ppath_agenda, ws.ppath_agenda__FollowSensorLosPath )\n",
    "ws.Copy( ws.ppath_step_agenda, ws.ppath_step_agenda__GeometricPath )\n",
    "@arts_agenda\n",
    "def geo_pos_agendaPY(ws):\n",
    "    ws.geo_posEndOfPpath()\n",
    "ws.Copy( ws.geo_pos_agenda, geo_pos_agendaPY)  ## new one, set by patrick in his cfile\n",
    "\n",
    "# define absorbing species and sensor (here using metmm library, used again below)\n",
    "ws.abs_speciesSet(species=[\"H2O-PWR98\",\"O2-PWR93\",\"N2-SelfContStandardType\"])#,\"liquidcloud-ELL07\"])\n",
    "ws.abs_lines_per_speciesSetEmpty()\n",
    "\n",
    "ws.stokes_dim = npol     # to get V and H pol set to 2\n",
    "ws.iy_unit = \"PlanckBT\"  # equivalent: ws.StringSet( iy_unit, \"PlanckBT\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up atmosphere and surface variables:\n",
    "ws.atmosphere_dim = 3  # 3D atmosphere\n",
    "#p = np.array([1015.,950.,800.])*100.0 #keep it simple, otherwise slower fwd model -- IGNORES LOTS OF WV!!!\n",
    "p = np.array([1015.,975.,950.,925.,900.,850.,800.,750.,700.,650.,600.,550.,500.,400.,300.,200.,100.])*100.0\n",
    "ws.p_grid = p[:] \n",
    "ws.AtmRawRead( basename = \"planets/Earth/Fascod/tropical/tropical\") #tropical atmosphere assumed at first\n",
    "ws.AtmosphereSet3D()\n",
    "\n",
    "ws.lat_grid = np.copy( la_grid[0:nlat] + np.diff(la_grid)*.5 ) # to avoid mismatch between ARTS and pcolormesh\n",
    "ws.lon_grid = np.copy( lo_grid[0:nlon] + np.diff(lo_grid)*.5 ) #  i.e. getting lat/lon midpoints of boxes\n",
    "nala, nalo = np.size(ws.lat_grid.value), np.size(ws.lon_grid.value) # size of ARTS lat/lon grid\n",
    "\n",
    "ws.AtmFieldsCalcExpand1D()  # set to given p_grid or z_grid\n",
    "\n",
    "#ws.vmr_field.value[0,:,:,:] *= 0.50 # try decreasing water vapor by X\n",
    "\n",
    "# if using coarse atmosphere but wanting accurate ray tracing:\n",
    "#ws.ppath_lmax = 350.0  # set maximum distance between points when computing absorption along path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in reanalysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumb coding but eh\n",
    "ns, ew = ['N','S',''], ['E','W','']\n",
    "def posneg(inpu):\n",
    "    if inpu>0:\n",
    "        return 0\n",
    "    if inpu<0:\n",
    "        return 1\n",
    "    if inpu==0:\n",
    "        return 2\n",
    "    \n",
    "# string defining ARTS grid area and spacing used to interpolate ERA5 data\n",
    "lat1,lat2 = np.min(ws.lat_grid), np.max(ws.lat_grid)\n",
    "lon1,lon2 = np.min(ws.lon_grid), np.max(ws.lon_grid)\n",
    "space = str(round(reso,2))+'x'+str(round(resa,2))+'y'\n",
    "sgrid = str(abs(round(lat1,1)))+ns[posneg(lat1)] +'-'+ str(abs(round(lat2,1)))+ns[posneg(lat2)] +'_'+ \\\n",
    "        str(abs(round(lon1,1)))+ew[posneg(lon1)] +'-'+ str(abs(round(lon2,1)))+ew[posneg(lon2)] + '_'+space\n",
    "print(sgrid)\n",
    "          \n",
    "from pathlib import Path\n",
    "e5fi = Path('arrs/e5-wi_'+sgrid+'.npy') # assume if one era5 npy file exists (not) at that grid, they all do (not)\n",
    "if e5fi.is_file():\n",
    "    print('interpolated era5 files exist already')\n",
    "else:\n",
    "    print('interpolating era5 data to ARTS grid')\n",
    "    # read in original era5 file, interpolate to ARTS grid and region of interest\n",
    "    # using ERA5 here. currently just a static date/time since only considering one orbit, but\n",
    "    #  could utilize the era5 python data client later to pull in data on the fly\n",
    "\n",
    "    #epaf = '/home/dudavid/Dendrite/Dendrite/UserAreas/Dave/EC/wv1609/'\n",
    "    epaf = 'era5/' # local dir \n",
    "    era5p = epaf+'p_vars_210916_15Z.nc'\n",
    "    era5s = epaf+'sfc_vars_210916_15Z.nc'\n",
    "    # fields are [time, levels, lat, lon] , so [1,17,721,1440] the way it was downloaded\n",
    "    # this file is 15Z, near the equator crossing time of the same 210916 orbit\n",
    "\n",
    "    era = Dataset(era5p)\n",
    "    era_q = np.array(era['q'][0,:,:,:])  # for some reason they are masked arrays unless explicitly set as np.array?\n",
    "    era_mr = era_q / (1-era_q)\n",
    "    era_t = np.array(era['t'][0,:,:,:])\n",
    "    print(np.shape(era_t))\n",
    "    elo = np.array(era['longitude'][:])\n",
    "    ela = np.array(era['latitude'][:])\n",
    "\n",
    "    eras = Dataset(era5s)\n",
    "    era_wi = np.array( eras['u10'][0,:,:]**2 + eras['v10'][0,:,:]**2 )**0.5  # get wind speed from vectors [m/s]\n",
    "    #era_td = eras['d2m'][:]  # 2m dewpoint temperature [K]   --- not being used right now!\n",
    "    era_wd = np.array(eras['dwi'][0,:,:])  # direction of 10m wind [degrees from N]\n",
    "    era_ts = np.array(eras['t2m'][0,:,:])  # 2m air temperature [K]\n",
    "    era_sp = np.array(eras['msl'][0,:,:])  # mean sea level pressure [Pa]\n",
    "    era_ss = np.array(eras['sst'][0,:,:])  # SST [K]\n",
    "    era_wd[era_wd<0] = 0.0   # for missing values, set to 0 (N)\n",
    "    \n",
    "    # shift grids from 0,360 lon to -180,180 lons:\n",
    "    elo    = np.concatenate( (elo[720:]-360, elo[:720]))\n",
    "    era_wi = np.concatenate( (era_wi[:,720:], era_wi[:,:720]) ,axis=1)\n",
    "    era_wd = np.concatenate( (era_wd[:,720:], era_wd[:,:720]) ,axis=1)\n",
    "    era_ts = np.concatenate( (era_ts[:,720:], era_ts[:,:720]) ,axis=1)\n",
    "    era_ss = np.concatenate( (era_ss[:,720:], era_ss[:,:720]) ,axis=1)\n",
    "    era_sp = np.concatenate( (era_sp[:,720:], era_sp[:,:720]) ,axis=1)\n",
    "    \n",
    "    from scipy.interpolate import interp2d \n",
    "    print(era_wi.shape)\n",
    "    tag = interp2d(elo, ela, era_wi)  # 'to arts grid' obj\n",
    "    np.save( 'arrs/e5-wi_'+sgrid,  tag(ws.lon_grid.value, ws.lat_grid.value) )\n",
    "    tag = interp2d(elo, ela, era_wd)  # 'to arts grid' obj\n",
    "    np.save( 'arrs/e5-wd_'+sgrid,  tag(ws.lon_grid.value, ws.lat_grid.value) )\n",
    "    tag = interp2d(elo, ela, era_ts)  # 'to arts grid' obj\n",
    "    np.save( 'arrs/e5-ts_'+sgrid,  tag(ws.lon_grid.value, ws.lat_grid.value) )\n",
    "    tag = interp2d(elo, ela, era_sp)  # 'to arts grid' obj\n",
    "    np.save( 'arrs/e5-sp_'+sgrid,  tag(ws.lon_grid.value, ws.lat_grid.value) )\n",
    "    tag = interp2d(elo, ela, era_ss)  # 'to arts grid' obj\n",
    "    np.save( 'arrs/e5-ss_'+sgrid,  tag(ws.lon_grid.value, ws.lat_grid.value) )\n",
    "    nze = 17 # p levels defined above (era5 data downloaded on same p grid)\n",
    "    e5mr = np.zeros([nze,nala,nalo])\n",
    "    e5t = np.zeros([nze,nala,nalo])\n",
    "    for l in range(nze):\n",
    "        tag = interp2d(elo, ela, np.concatenate( (era_mr[l,:,720:],era_mr[l,:,:720]),axis=1 )) # also shift lons\n",
    "        e5mr[l,:,:] = tag(ws.lon_grid.value, ws.lat_grid.value)\n",
    "        tag = interp2d(elo, ela, np.concatenate( (era_t[l,:,720:],era_t[l,:,:720]),axis=1 )) # also shift lons\n",
    "        e5t[l,:,:]  = tag(ws.lon_grid.value, ws.lat_grid.value)\n",
    "    np.save( 'arrs/e5-t_'+sgrid, e5t )\n",
    "    np.save( 'arrs/e5-mr_'+sgrid, e5mr )\n",
    "    \n",
    "# can plot up era5 data to verify interpolation and subselection like this:\n",
    "#gcartomap(era_wi,ela,elo,0,15)#,region=[-90,90,-180,180])\n",
    "#wi_in = np.load('arrs/e5-wi_'+sgrid+'.npy')\n",
    "#gcartomap(wi_in,ws.lat_grid.value,ws.lon_grid.value, 0,15,region=[np.min(la_grid), np.max(la_grid),\n",
    "#           np.min(lo_grid), np.max(lo_grid)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set ARTS background (from ERA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Surface properties -- define ocean surface for simulations\n",
    "\n",
    "s_names = [\"Water skin temperature\",\"Wind speed\",\"Wind direction\",\"Salinity\"]\n",
    "s_data = np.zeros([len(s_names), nala, nalo])  ## page/row/col\n",
    "print('s_data shape: ',s_data.shape)\n",
    "\n",
    "#tmean                   = 292.0 # sst mean\n",
    "#wsp                     = 6.3   # 10m wind speed\n",
    "# read era5 data directly from saved np arrays, already interpolated to ARTS grid defined\n",
    "s_data[0,:,:] = np.load('arrs/e5-ts_'+sgrid+'.npy') #### try using T2m instead of \n",
    "#s_data[0,:,:] = np.load('arrs/e5-ss_'+sgrid+'.npy') ### ERA5 SST...\n",
    "s_data[1,:,:] = np.load('arrs/e5-wi_'+sgrid+'.npy')\n",
    "s_data[2,:,:] = np.load('arrs/e5-wd_'+sgrid+'.npy')   # in deg from N, as expected by ARTS\n",
    "s_data[3,:,:] = 0.034                       # token salinity value\n",
    "\n",
    "# write these to ARTS variables \n",
    "ws.Copy(ws.surface_props_names, s_names)\n",
    "ws.Copy(ws.surface_props_data, s_data)\n",
    "ws.MatrixSetConstant(ws.z_surface, nala, nalo, 0.0) # explicitly set the surface to 0m altitude\n",
    "\n",
    "\n",
    "# also profile information from ERA5:\n",
    "ws.t_field.value[:,:,:] = np.load('arrs/e5-t_'+sgrid+'.npy')[::-1,:,:]\n",
    "ws.vmr_field.value[0,:,:,:] = np.load('arrs/e5-mr_'+sgrid+'.npy')[::-1,:,:]*28.966/18.016 # convert MR to VMR\n",
    "ws.p_grid.value[0] = np.mean(np.load('arrs/e5-sp_'+sgrid+'.npy')[:,:]) # just take mean from scene? not huge deal..\n",
    "\n",
    "plt.pcolor(s_data[0,:,:]) # quick look for SST\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.pcolor(s_data[1,:,:]) # quick look for wsp\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define antenna pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set and calculate some basic variables for antenna and scanning\n",
    "\n",
    "### All will be replaced or superseded if using L1R data as input\n",
    " \n",
    "#zsat   = 699.7e3      # Satellite altitude [m] -- can read this in later\n",
    "#vsat   = 6.76e3       # Satellite velocity [m/s]\n",
    "#dt     = 2.6e-3       # Integration time [s] -- 2.6ms for low freqs, 1.3ms for 89GHz\n",
    "#rpm    = 40           # Rotations per minute -- same for AMSR-E and AMSR2\n",
    "# \n",
    "## nautical mile constant?  1852m = 1nmi\n",
    "#m2deg  = 1/(60*1852)                          # Conversion from m to latitude\n",
    "#dang   = dt * 360 * rpm / 60                  # Angular distance between samples\n",
    "#dlat   = m2deg * vsat * 60 / rpm              # Latitude distance between scans\n",
    "#print(dang,dlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "width  = 1.8           # Max half-width of antenna simulated (if HPBW is 1.8deg, width=2 covers 2x the HPBW)\n",
    "resol  = 0.01          # angular resolution (same in zenith/azimuth)\n",
    "# note: for a given angular resolution (might vary), width=2 yields 99.96% of total power @6GHz \n",
    "#   compared to width=20 and 99.99999% @10GHz, width=1.5 yields 99.0 and 99.994 @ 6,10\n",
    "#resol, width = dang/(angfac*5), awidth # set here instead of calling func\n",
    "print('angular resolution of response grid [deg]: ',resol)\n",
    "\n",
    "adata = [[ 6.925e9 ,  1.80],   # AMSR2 center frequency and beamwidth (deg)\n",
    "         [ 7.300e9 ,  1.80],   # assumed the same for V/H polarizations\n",
    "         [ 10.65e9 ,  1.20],\n",
    "         [ 18.70e9 ,  0.65],\n",
    "         [ 23.80e9 ,  0.75],\n",
    "         [ 36.50e9 ,  0.35],\n",
    "         [ 89.00e9 ,  0.15]] \n",
    "\n",
    "adata = np.array(adata, order=\"C\").transpose()[:,fsub] # choose selected frequencies (set above)\n",
    "\n",
    "# define zenith, azimuth grid on the ground (relative to bore sight) -- assumed to be square, 2xWidth wide\n",
    "x  = np.arange( -width, width+resol, resol )  #this gives angular antenna response points in za,aa\n",
    "x2 = x**2   # since assumed origin is 0, do squaring here \n",
    "nf, nx = len(adata[0,:]), np.size(x)   # num frequencies, size of antenna grid\n",
    "\n",
    "#print(ch_str)\n",
    "\n",
    "from typhon.arts.griddedfield import GriddedField4\n",
    "gf4 = GriddedField4()   # ARTS variable type, found in typhon\n",
    "gf4.name = 'AMSR2 antenna response'\n",
    "gf4.gridnames =  [ 'Polarisation', 'Frequency', 'Zenith angle', 'Azimuth angle' ]\n",
    "###  note: za & aa are equally spaced, and we're treating V/H as having identical responses\n",
    "if npol==2: gsp=\"1\" \n",
    "else: gsp=\"0\"\n",
    "gf4.grids     = [ [gsp], adata[0,:], x, x ]\n",
    "gf4.dataname  = 'Response'\n",
    "gf4.data      = np.zeros([ 1, nf, nx, nx ], order=\"C\")\n",
    "print('size of antenna_reponse grid: ',gf4.data.shape)\n",
    "\n",
    "for i in range(nf):\n",
    "    si = adata[1,i] / (2*np.sqrt(2*np.log(2)))  # calculate standard deviation first, based on HPBW\n",
    "    gf4.data[0,i,:,:] = np.exp( - np.tile(x2,[nx,1])/si**2 - np.tile(x2,[nx,1]).transpose()/ si**2 )\n",
    "\n",
    "\n",
    "# frequency grid of simulation is defined according to sensor setup above (may change with use of metmm)\n",
    "f_grid = np.copy(gf4.grids[1])  # array with each frequency (not channel)\n",
    "\n",
    "ws.f_grid.value = f_grid\n",
    "print('f_grid: ',ws.f_grid.value)\n",
    " \n",
    "#plt.pcolormesh(gf4.data[0,0,:,:])# to plot antenna pattern of one freq\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define bore sights for one scan, angular grid for fwd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## Determine bore-sight angles to use for one scan \n",
    "# \n",
    "## psteps defines the pixels across one scan\n",
    "#psteps   = np.arange(-(npix-1)/2, (npix-1)/2 +1, 1)  #  if npix is odd then middle boresight is in the middle\n",
    "## ssteps defines position of spacecraft for each scan, evenly distributed around the middle scan\n",
    "#ssteps   = np.arange(-(nscans-1)/2, (nscans-1)/2 +1, 1) \n",
    "#\n",
    "##  center line of sight... defined as AMSR2 EIA and 0 azimuth angle\n",
    "#los0    = [ 180-47.5, 0 ]  # 47.5 is off-nadir angle of AMSR2, 180 for looking straight down\n",
    "#\n",
    "## bsights defines the bore sight zenith and azimuth angles across the scan \n",
    "###-- just for one scan, assumed to be same / repeated for all scans\n",
    "#bsights = np.array([ np.repeat(los0[0],npix), los0[1]+dang*psteps ]).transpose()  # size: [npix,2]\n",
    "#print('bsights:',(bsights[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using real data, but while exact EIA is given in L1R the sensor zenith angle is \n",
    "#  assumed 47.0deg for all points. azimuth is given\n",
    "\n",
    "# los0 defines zenith and azimth of center point, but azimuth will be overridden by L1R data\n",
    "#  [bore sights defined from middle scan, for all chosen pixels]\n",
    "#bsights = np.array([np.repeat(180-47.5,npix), eaz[pxt:pxp,scm]]).transpose() # size of [npix,2] \n",
    "# look at L1 documentation for how earth azimuth is defined (but i think it's 180 off the ARTS definition...)\n",
    "#print(bsights)\n",
    "# assumes azimuth angle sequence will be same for each scan, and note that 89GHz B-scan is 180-47.0\n",
    "\n",
    "# try new calculation of boresights, using a derived azimuth angle (not L1R one)\n",
    "# do this by defining sensor_pos for central scan, target as L1R la/lo on ground, and ARTS function to draw line\n",
    "ws.sensor_pos = np.hstack([np.repeat(altitude,  npix+1).reshape(npix+1,1),\n",
    "                           np.repeat(SClat[scm],npix+1).reshape(npix+1,1),\n",
    "                           np.repeat(SClon[scm],npix+1).reshape(npix+1,1)])\n",
    "targ_pos = np.hstack([ np.repeat(0,npix+1).reshape(npix+1,1),\n",
    "                       ala[pxt:pxp+1,scm].reshape(npix+1,1),\n",
    "                       alo[pxt:pxp+1,scm].reshape(npix+1,1)])\n",
    "ws.sensor_losGeometricFromSensorPosToOtherPositions( target_pos=targ_pos )\n",
    "print(ws.sensor_los.value)\n",
    "bs = np.copy(ws.sensor_los.value[:,:]) # all [npix+1,3]\n",
    "bsights = np.copy(ws.sensor_los.value[:npix,:]) # just [npix,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define angular grid for pencilbeam calculations\n",
    "\n",
    "# can use 'dang' or not, but benefit of using it is having RT calc at each boresight (i.e. antenna beam max)\n",
    "#  in azimuth direction. if using 'dang' for zenith too then it's an evenly spaced angular grid.\n",
    "\n",
    "n = np.floor( width * angfac )  # angular width * angular samples per deg (same width as antenna pattern grid)\n",
    "#za_grid = np.array( 180-47.5 +  np.arange(-n,n+1)/angfac)  ## effectively taking 'width' on either side of 0\n",
    "#\n",
    "#mid_aa_grid = np.interp(np.arange(0,npix+1/angfac,1/angfac), np.arange(0,npix+1), eaz[pxt:pxp+1,scm] )\n",
    "## need to add on az values on outside to get edges off boresight extremes\n",
    "#aa_grid = np.concatenate(( \n",
    "#    (eaz[pxt,scm] - np.arange(np.diff(mid_aa_grid[0:2]),width,np.diff(mid_aa_grid[0:2])))[::-1],\n",
    "#    mid_aa_grid,\n",
    "#    (eaz[pxp,scm] + np.arange(np.diff(mid_aa_grid[-3:-1]),width,np.diff(mid_aa_grid[-3:-1]))) ))\n",
    "\n",
    "za_grid = np.array( bsights[0,0] +  np.arange(-n,n+1)/angfac)  ## effectively taking 'width' on either side of 0\n",
    "mid_aa_grid = np.interp(np.arange(0,npix+1/angfac,1/angfac), np.arange(0,npix+1), bs[:npix+1,1] )[:-1]\n",
    "print(mid_aa_grid)\n",
    "# need to add on az values on outside to get edges off boresight extremes\n",
    "aa_grid = np.concatenate(( \n",
    "    (bs[0,1] + np.arange(abs(np.diff(mid_aa_grid[0:2])), width, abs(np.diff(mid_aa_grid[0:2])))[::-1]),\n",
    "    mid_aa_grid,\n",
    "    (bs[npix,1] - np.arange(abs(np.diff(mid_aa_grid[-3:-1])),width,abs(np.diff(mid_aa_grid[-3:-1])))) ))\n",
    "\n",
    "#aa_grid[aa_grid>180] -=360 # limit to -180,180 domain (done internally in ARTS?)\n",
    "#aa_grid[aa_grid<-180]+=360\n",
    "print('za grid: ',za_grid)\n",
    "print('az grid: ',aa_grid)\n",
    "\n",
    "# older code:\n",
    "#print(n)\n",
    "#en = np.floor( angfac * ( bsights[-1,1] + width ) / dang ) # azimuth of boresight at edge +width\n",
    "#print(en)\n",
    "#n = np.floor( angfac * width / dang )  # number per degree * angular width / distance between boresights\n",
    "#za_grid = np.array( los0[0] + (dang/angfac) * np.arange(-n,n+1))  # zenith angle grid\n",
    "#en = np.floor((bsights[-1,1]+width)*angfac) ## yields az from boresight at edge plus 'width' (could prob do width/2)\n",
    "#aa_grid = np.array( los0[1] + (dang/angfac) * np.arange(-en,en+1))  # azimuth angle grid\n",
    "#aa_grid = np.array( los0[1] + np.arange(-en,en+1/angfac)/angfac)  # azimuth angle grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sensor_pos (position), sensor_los (line of sight) for ARTS\n",
    "\n",
    "# should this get called? not yet -- will give bsights from designated positions, draw a line \n",
    "#ws.sensor_losGeometricFromSensorPosToOtherPositions(ws.target_pos=np.array([]))\n",
    "\n",
    "#ws.sensor_pos = np.array([[ altitude ]]) # set to correct satellite altitude [m]\n",
    "#los0=np.array([133,0])\n",
    "#ws.sensor_los = np.tile(los0, [nscans,1])  # sensor line of sight for each scan\n",
    "\n",
    "\n",
    "#shift   = 7.3   # Latitude shift to centre calculations around lat0\n",
    "\n",
    "# sensor_pos should be columns of altitude, SClat, SClon  ###  \n",
    "#   with real data these come from L1R files and transform X/Y/Z to lat/lon\n",
    "#   currently one alt/sclat/sclon for each scan (i.e. each of nscans)\n",
    "\n",
    "ws.sensor_pos = np.hstack([np.repeat(altitude, nscans).reshape(nscans,1),\n",
    "                           SClat[sct:scp].reshape(nscans,1),\n",
    "                           SClon[sct:scp].reshape(nscans,1)])\n",
    "                          #np.array(lat0 - shift + dlat * ssteps).reshape(nscans,1), \n",
    "                          #np.repeat(lon0, nscans).reshape(nscans,1)])\n",
    "print(np.shape(ws.sensor_pos))\n",
    "targ_pos = np.hstack([ np.repeat(0,nscans).reshape(nscans,1),\n",
    "                       ala[pxm,sct:scp].reshape(nscans,1),\n",
    "                       alo[pxm,sct:scp].reshape(nscans,1)])\n",
    "ws.sensor_losGeometricFromSensorPosToOtherPositions( target_pos=targ_pos )\n",
    "#                    #ws.sensor_pos, atmosphere_dim=3, lat_grid=ws.lat_grid,\\\n",
    "#                    #lon_grid=ws.lon_grid, sensor_los=ws.sensor_los,\\\n",
    "#                    refellipsoid=ws.refellipsoidEarth(\"WGS84\"), targ_pos)\n",
    "# \tsensor_los, atmosphere_dim, lat_grid, lon_grid, refellipsoid, sensor_pos, target_pos )\n",
    "\n",
    "print(ws.sensor_los.value) #,np.shape(ws.sensor_los))\n",
    "\n",
    "#ws.sensor_los = np.hstack([np.repeat(180-47.5,nscans).reshape(nscans,1),\n",
    "#                           np.repeat(0,nscans).reshape(nscans,1)])\n",
    "#                          #(eaz[pxm,sct:scp]).reshape(nscans,1)])\n",
    "#print(ws.sensor_los.value) #,np.shape(ws.sensor_los))\n",
    "\n",
    "#print(ws.sensor_pos.value)\n",
    "#print(np.shape(ws.sensor_pos.value))  # should be alt, SClat, SClon for each of nscans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn off jacobian calc for primary yCalc, keep clear-sky (non-scattering) setup\n",
    "ws.jacobianOff()\n",
    "ws.cloudboxOff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the surface agenda for ARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define surface agenda (need transmittance to run FASTEM):\n",
    "varnam = [\"Optical depth\"]\n",
    "ws.Copy(ws.iy_aux_vars, varnam)\n",
    "ws.VectorCreate( \"transmittance\" )\n",
    "#ws.transmittance = np.ones( ws.f_grid.value.shape ) * 0.0  #  FOR TESTING ONLY!!!!!\n",
    "\n",
    "@arts_agenda\n",
    "def iy_surface_agendaPY(ws):\n",
    "    ws.specular_losCalc()\n",
    "    # if wanting to test transmittance values, comment out next 3 calls and set outside agenda...\n",
    "    #  if ppathCalc is called and uses assumed inputs there are problems, so specify all!\n",
    "    ws.ppathCalc(ws.ppath_agenda, ws.ppath_lmax, \n",
    "       ws.ppath_lraytrace, ws.atmgeom_checked, ws.t_field, ws.z_field, \n",
    "       ws.vmr_field, ws.f_grid, ws.cloudbox_on, ws.cloudbox_checked, \n",
    "       ws.ppath_inside_cloudbox_do, ws.rtp_pos, ws.specular_los, ws.rte_pos2 )\n",
    "    ws.iyEmissionStandard()\n",
    "    ws.transmittanceFromIy_aux(transmittance=ws.transmittance)\n",
    "    ws.SurfaceFastem( transmittance = ws.transmittance, fastem_version=6 ) \n",
    "    ws.iySurfaceRtpropCalc()\n",
    "    \n",
    "ws.Copy(ws.iy_surface_agenda, iy_surface_agendaPY) # copy python-defined agenda to ARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform checks and set up mblock grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform some checks:\n",
    "ws.abs_xsec_agenda_checkedCalc()\n",
    "ws.propmat_clearsky_agenda_checkedCalc()\n",
    "ws.atmfields_checkedCalc( bad_partition_functions_ok = 1 )\n",
    "ws.atmgeom_checkedCalc()\n",
    "ws.cloudbox_checkedCalc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create vector mblock_reference_los and matrix mblock_target_los (to be used by DiffZaAa)\n",
    "#mblock_reference_los = np.array([180-47.5, eaz[pxm,scm]])   # choose angle from mid scan as reference point\n",
    "ref_angs = bsights[int(npix/2),:]  # middle pix, middle scan\n",
    "print('reference angles (for mblock, antenna):', ref_angs)\n",
    "mblock_reference_los = ref_angs    # choose angle from mid scan as reference point\n",
    "\n",
    "# want to have all individual angle pairs distinct... so repeat one serially and one n times\n",
    "mblock_target_los = np.array(np.transpose( np.stack( \n",
    "                    [ np.repeat(za_grid, aa_grid.size) , \n",
    "                      np.tile(  aa_grid, za_grid.size) ]) ) , order = \"C\")\n",
    "## need to specify 'C' order or else stride is wrong and array values are read wrong by ARTS!\n",
    "\n",
    "# take the differences of zenith and azimuth angles to convert to chosen angle space\n",
    "ws.DiffZaAa(ws.mblock_dlos_grid, mblock_reference_los, mblock_target_los)\n",
    "\n",
    "#print(np.shape(ws.mblock_dlos_grid.value))\n",
    "#print(np.diff(ws.mblock_dlos_grid.value[0:10,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Calculate the sensor response function from the antenna response\n",
    "ws.VectorCreate(\"antenna_reference_los\")\n",
    "ws.MatrixCreate(\"antenna_target_los\")\n",
    "#ws.antenna_reference_los = np.array([180-47.5, eaz[pxm,scm]])\n",
    "ws.antenna_reference_los = ref_angs\n",
    "ws.antenna_target_los = np.array(bsights, order=\"C\")\n",
    "ws.DiffZaAa(ws.antenna_dlos, ws.antenna_reference_los, ws.antenna_target_los)\n",
    "\n",
    "# define sensor... done above, but apply gf4 typhon/arts object to antenna_response:\n",
    "ws.sensor_norm = 1\n",
    "ws.antenna_dim = 2\n",
    "ws.antenna_response = gf4 \n",
    "\n",
    "ws.sensor_responseInit()\n",
    "\n",
    "## flipping from first two stokes components (I/Q) to V/H-pol Tb here... \n",
    "ws.instrument_pol = [5,6] # indices for V,H (1,2 are I,Q)\n",
    "\n",
    "ws.sensor_responsePolarisation()\n",
    "ws.sensor_responseAntenna()\n",
    "ws.sensor_checkedCalc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first, set up the retrieval grid, which is independent from fwd model and\n",
    "#  independent from the background (lat/lon/alt) grid if chosen\n",
    "\n",
    "utx, uty = 2.0, 1.4      # grid defined radially out from center, so these are half-widths\n",
    "r_lims = [lat0-uty, lat0+uty, lon0-utx, lon0+utx] # lat/lon limits  (box in middle of overall box)\n",
    "\n",
    "r_lon  = np.arange(r_lims[2], r_lims[3]+r_reso, r_reso)\n",
    "r_lat  = np.arange(r_lims[0], r_lims[1]+r_resa, r_resa)\n",
    "\n",
    "print('retrieval grid resolution: ', r_resa, r_reso)\n",
    "#print('retrieval lon grid: ', r_lon)\n",
    "#print('retrieval lat grid: ', r_lat)\n",
    "\n",
    "# create grids of retrieval lats/lons for calculations of distance and plotting below\n",
    "ri, rj = np.meshgrid(r_lat, r_lon, indexing='ij')  \n",
    "print(ri.shape,rj.shape)\n",
    "# and the same for original ARTS background grid\n",
    "gi, gj = np.meshgrid(ws.lat_grid, ws.lon_grid, indexing='ij')\n",
    "print(gi.shape,gj.shape)\n",
    "\n",
    "# calculate 'x_back' which is mapping s_data to the retrieval grid\n",
    "x_back = np.zeros([nrvar, r_lat.size, r_lon.size])\n",
    "print('rgrid dimensions: ',x_back.shape)\n",
    "from scipy.interpolate import interp2d \n",
    "f = interp2d(ws.lon_grid, ws.lat_grid, s_data[0,:,:]) #, kind='cubic')\n",
    "x_back[0,:,:] = f(r_lon,r_lat)\n",
    "if nrvar>1:\n",
    "    f = interp2d(ws.lon_grid, ws.lat_grid, s_data[1,:,:]) #, kind='cubic')\n",
    "    x_back[1,:,:] = f(r_lon,r_lat)\n",
    "\n",
    "#for r in range(nrvar):\n",
    "#    plt.matshow(x_back[r,:,:], cmap=cmo.dense)\n",
    "#    plt.colorbar()\n",
    "#    plt.matshow(s_data[r,:,:], cmap=cmo.dense)\n",
    "#    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## map x_back to background grid in ARTS, check it looks right\n",
    "#ws.x = np.concatenate([bla.flatten(order = \"f\") for bla in x_back])\n",
    "#print(ws.x.value)\n",
    "#ws.x2artsAtmAndSurf()\n",
    "#plt.figure(figsize=[14,7])\n",
    "#plt.pcolor(ws.surface_props_data.value[0,:,:])\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set prior and covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# set Xa (formally set in ARTS below)\n",
    "xa = np.zeros([nrvar, r_lat.size, r_lon.size]) # a priori matrix, should be similar to s_data\n",
    "print('Xa shape: ', xa.shape)\n",
    "xa[0,:,:] = np.mean(x_back[0,:,:]) #+np.random.normal(0,1,[r_lat.size,r_lon.size]) #+ 0.07  # set constant, or maybe some perturbation\n",
    "if nrvar>1:\n",
    "    xa[1,:,:] = np.mean(x_back[1,:,:]) #+np.random.normal(0,1,[r_lat.size,r_lon.size])\n",
    "\n",
    "# then observation errors (first just sensor noise)\n",
    "se_cov = np.diag(np.tile(np.repeat((noise_fac*nedt)**2,2), npix*nscans ))  # if nedt constant for all chans\n",
    "#se_cov = np.diag(np.tile(np.repeat(nedt**2,2), npix*nscans ))  # if nedt constant for all chans\n",
    "print('Se cov shape: ', se_cov.shape) # square matrix of side length npix*nscans*nf*2\n",
    "\n",
    "# calculate distances between points on retrieval grid for distance-dependent correlations in sx_cov\n",
    "#   \"Convert a vector-form distance vector to a square-form distance matrix\"\n",
    "#dists = squareform( pdist( np.array([ri.flatten(), rj.flatten()]).transpose() )) \n",
    "dists = squareform( pdist( np.array([ri.flatten(order=\"F\"), rj.flatten(order=\"F\")]).transpose() )) \n",
    "# ri/rj flattened in F order because all 2D arrays interpreted in 1D by arts get reshaped internally!\n",
    "\n",
    "# can try making decorrelation lengths gradient-dependent (wait for a non-flat prior for testing!):\n",
    "#rygrad, rxgrad = np.gradient(xa[:,:,0])\n",
    "#pseudo_grad = 0.5 * (np.abs(rygrad)*r_resa + np.abs(rxgrad)*r_reso)   # should be array of xa size\n",
    "## abs values, add gradients in x and y by retr grid resolution\n",
    "#deco_grid = np.ones_like(dists)*deco\n",
    "#deco_grid[pseudo_grad > 0] *= np.exp(-1.0*pseudo_grad[pseudo_grad>0])  \n",
    "## ad hoc, weight decorr length to be shorter if there's a non-zero gradient in apriori \n",
    "\n",
    "\n",
    "# then a priori state errors (Sa or Sx)\n",
    "len_rgrid = r_lat.size*r_lon.size # total number of points in retrieval grid\n",
    "nsx = np.sum(range(nrvar+1)) # 'triangle number' determines # of matrix blocks that define sx_cov\n",
    "\n",
    "# blocks of larger, sparse Sa unless the off-off diagonal elements are filled in (SST/wsp covariances)\n",
    "sx_cov = np.zeros([len_rgrid,len_rgrid, nsx], order=\"C\")\n",
    "\n",
    "# common for both:\n",
    "sx_cov[:,:,0] = np.diag( np.tile(sx_sst**2, len_rgrid)) \n",
    "corrs_sst = np.exp(-dists/deco_sst) \n",
    "sx_cov[:,:,0]  = sx_sst * sx_sst * corrs_sst[:,:]  # FOR NON-LOC-DEP SX_SST    i.e.  cov = r * sig * sig\n",
    "#for x in range(len_rgrid):\n",
    "#    for y in range(len_rgrid):\n",
    "#        sx_cov[x,y,0]  = sx_sst * sx_sst * corrs_sst[x,y]  # i.e.  cov = r * sig * sig\n",
    "    \n",
    "if nrvar==2:  # wind and cross-correlations\n",
    "    corrs_wsp = np.exp(-dists/deco_wsp)\n",
    "    sx_cov[:,:,1] = np.diag( np.tile(sx_wsp**2, len_rgrid)) \n",
    "    sx_cov[:,:,1] = sx_wsp * sx_wsp * corrs_wsp[:,:]  # if sx_wsp is NOT function of loc, calc here\n",
    "    #sx_cov[:,:,2] = sx_wsp * sx_sst * corrs_wsp[:,:] * xcorr  # cross correlations -- again, NOT f(loc)\n",
    "    #for x in range(len_rgrid):\n",
    "    #    for y in range(len_rgrid):\n",
    "    #        sx_cov[x,y,1]  = sx_wsp * sx_wsp * corrs_wsp[x,y]  # if sx_wsp is function of loc, include here\n",
    "    #        # cross correlations! (w/ spatial corr from 1):\n",
    "    #        sx_cov[x,y,2]  = sx_wsp * sx_sst * corrs_wsp[x,y] * xcorr  # cross correlations\n",
    "\n",
    "#for rv in range(nsx):   # can plot each a priori covariance matrix block:\n",
    "#    plt.matshow(sx_cov[:,:,rv])\n",
    "#    plt.colorbar()\n",
    "#    plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize retrieval setup in ARTS, run yCalc for syn. observation vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first forward model call, to calculate synthetic observation vector\n",
    "#print('running yCalc')\n",
    "#ws.yCalc() \n",
    "#print('done initial yCalc')\n",
    "\n",
    "tb_obs = tb_cal[pxt:pxp, sct:scp, :]  # input observed TBs with calibration offsets applied\n",
    "tb_obs_reshape = np.transpose(tb_obs,axes=(2,1,0)) # so to [chans,scans,pix] # from [px,sc,ch] -> [ch,sc,px]\n",
    "print(tb_obs_reshape.shape)\n",
    "tbo_flat = np.zeros([nscans*npix*nch])\n",
    "\n",
    "for c in range(nf*2):\n",
    "    tbo_flat[c::nf*2] = tb_obs_reshape[c,:,:].flatten() #order=\"F\")\n",
    "    \n",
    "#tb_back = np.zeros([nscans,npix,nf*2]) \n",
    "#for c in range(nf*2):\n",
    "#    tb_back[:,:,c] = tbo_flat[c::nf*2].reshape(nscans,npix)\n",
    "\n",
    "# define first guess for x in iteration. if no ws.x is set before OEM runs, it uses ws.xa.\n",
    "#### flatten xa/whatever in correct order for ARTS\n",
    "firstg = np.concatenate([bla.flatten(order = \"f\") for bla in xa])\n",
    "ws.x = firstg\n",
    "#ws.x = ws.xa #firstg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# formally set covarainces and prior for ARTS OEM:\n",
    "#  [use numpy copy to make damn sure arrays are separate in memory]\n",
    "\n",
    "ws.retrievalDefInit()  # initialize then add variables\n",
    "\n",
    "for r in range(nrvar):\n",
    "    ws.retrievalAddSurfaceQuantity( g1=r_lat, g2=r_lon, quantity=s_names[r])\n",
    "    ws.covmat_sxAddBlock(block = np.copy(np.squeeze(sx_cov[:,:,r])), i=r, j=r )  # for 'diagonal' blocks\n",
    "    \n",
    "# for cross-correlation blocks (if more than 2 vars, revisit this):\n",
    "#if nrvar>1:\n",
    "#    ws.covmat_sxAddBlock(block = np.copy(np.squeeze(sx_cov[:,:,2])), i=0, j=r )\n",
    "\n",
    "xa_forarts = np.concatenate([bla.flatten(order=\"F\") for bla in xa])  # ordering bullshit\n",
    "#xa_forarts = xa.flatten()\n",
    "ws.Copy(ws.xa, np.copy(xa_forarts))  # copy python xa to arts xa (everything collapsed to 1D)\n",
    "\n",
    "ws.covmat_seSet(np.copy(se_cov))\n",
    "\n",
    "ws.retrievalDefClose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jqs = ws.jacobian_quantities.value\n",
    "#rq =  jqs[0]\n",
    "#rq.grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the iteration agenda for ARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include 'clip' limits for retrieved variables\n",
    "from IPython.display import display, clear_output\n",
    "ic = 0\n",
    "#draw_fun = fig.canvas.draw\n",
    "plt.ion()\n",
    "#f, axs = plt.subplots(1, 2)\n",
    "#ax_sst, ax_w = axs\n",
    "#f.show()\n",
    "#\n",
    "#def update():\n",
    "#    f.canvas.draw()\n",
    "ceems = cmo.solar\n",
    "ceemb = cmo.balance\n",
    "\n",
    "@arts_agenda\n",
    "def inversion_iterate_agenda(ws):\n",
    "    ws.Ignore(ws.inversion_iteration_counter) # on simon's advice\n",
    "    \n",
    "    ws.xClip(ijq = 0, limit_low = 272.0, limit_high = 310.0)  #limits retrieval range of SST\n",
    "    if nrvar>1: ws.xClip(ijq = 1, limit_low = 0.5, limit_high = 20.0)  # limits wind_speed range\n",
    "        \n",
    "    ws.x2artsAtmAndSurf()  # map x to ARTS's variables -- THIS INVOLVES ARTS RESHAPE OF 1D to 3D FIELDS!\n",
    "    \n",
    "    x_iter = np.copy( ws.x.value ).reshape(nrvar, r_lat.size, r_lon.size) \n",
    "    xb_iter = np.copy( ws.surface_props_data.value ).reshape(4, nala, nalo) # on arts background grid\n",
    "    \n",
    "    ws.yCalc() \n",
    "\n",
    "    ws.Copy(ws.y_baseline, np.zeros([tbo_flat.size])) # kinda silly but necessary?\n",
    "    ws.VectorAddVector( ws.yf, ws.y, ws.y_baseline )  # add baseline term (need to create ws.yf)\n",
    "    \n",
    "    # this takes care of some fixes needed to get the jacobian right for iterative solutions:\n",
    "    ws.jacobianAdjustAndTransform()\n",
    "    \n",
    "    #\n",
    "    # Debug stuff: view fields as iteration progresses (will show up below OEM call)\n",
    "    \n",
    "    if np.mod(ic,2)==0:\n",
    "        f, (ax_sst, ax_w) = plt.subplots(1, 2, figsize=[18,7])\n",
    "    \n",
    "        ax_sst.set_title('SST, Iter='+str(ic//2))\n",
    "        ax_w.set_title('Wind, Iter='+str(ic//2))\n",
    "    \n",
    "        vch = 3 # cbar change value (both sst/wsp)\n",
    "        ax_sst.matshow(xb_iter[0,:,:]-s_data[0,:,:], vmin=-vch, vmax=vch, cmap=ceemb)\n",
    "        sms = plt.cm.ScalarMappable(cmap=ceemb, norm= plt.Normalize(vmin=-vch,vmax=vch))\n",
    "        sms._A = []\n",
    "        cbs = plt.colorbar(sms, ax=ax_sst)\n",
    "        if nrvar>1:\n",
    "            ax_w.matshow(xb_iter[1,:,:]-s_data[1,:,:], vmin=-vch, vmax=vch, cmap=ceemb)\n",
    "            smw = plt.cm.ScalarMappable(cmap=ceemb, norm= plt.Normalize(vmin=-vch,vmax=vch))\n",
    "            smw._A = []\n",
    "            cbw = plt.colorbar(smw, ax=ax_w)\n",
    "    \n",
    "        clear_output(wait=True)\n",
    "        display(f)  # outputs figure with x vectors to screen\n",
    "        #display('grid vars updating (unreshaped): ',ws.x.value) #x_iter[1,:,:])\n",
    "    \n",
    "    ic += 1\n",
    "\n",
    "ws.Copy(ws.inversion_iterate_agenda, inversion_iterate_agenda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call OEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_geo doesn't get initialized/set if a yCalc isn't called outside of ARTS OEM???\n",
    "ws.yCalc() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize y vector\n",
    "\n",
    "# disregard initial yCalc\n",
    "ws.Copy(ws.y, tbo_flat )  # copy to the ARTS y vector prior to running OEM.\n",
    "#ws.Copy(ws.y, tb_obs_reshape )  # copy to the ARTS y vector prior to running OEM\n",
    "\n",
    "if np.diagonal(se_cov).size != tb_obs.size:\n",
    "    print('Se covariance matrix and Tb vector not same size!')\n",
    "    aaaaandstop\n",
    "\n",
    "print('last gasp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call OEM:\n",
    "\n",
    "#  note: LM is better for non-linear problems and sfc retrieval should be very linear, so stick with GN/GN_CG\n",
    "ws.OEM(method=\"gn\",  # in one test LM took ~3x longer than GN!!\n",
    "    max_iter=5,\n",
    "    display_progress=1,\n",
    "    max_start_cost=1e5,\n",
    "    stop_dx=.4, # default is .01? -- this is to limit iterations, can dial back later for more exactness\n",
    "    lm_ga_settings=np.array([10.0,2.0,3.0,10000000000.0,1.0,1.0])) # only applicable if method='lm'\n",
    "\n",
    "ws.Print(ws.oem_errors, 0)  # print any errors to terminal\n",
    "\n",
    "endt = time. time()\n",
    "print('Completion time [s]:',endt - startt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### calculate averaging kernel (A matrix) and a posteriori \n",
    "# A matrix, S_o, S_s are all same size: [x,x] where x=npix*nscans*nretvar\n",
    "ws.avkCalc()\n",
    "Amat = ws.avk.value\n",
    "print('Trace of A Matrix: ', np.trace(Amat))   #Trace(A) yields DOF for signal.\n",
    "print('(out of possible points and vars:',np.diagonal(Amat).size)\n",
    "print('  ...so DOF per retr grid point: ',np.trace(Amat)/np.diagonal(Amat).size)\n",
    "\n",
    "ws.covmat_soCalc()\n",
    "post = ws.covmat_so.value  # posterior error covariance matrix\n",
    "ws.covmat_ssCalc()\n",
    "smoov = ws.covmat_ss.value  # posterior error covariance matrix\n",
    "# should be [x,x], providing uncertainties of each retrieved var, where x=npix*nscans*nretvar\n",
    " \n",
    "### ARTS method: \"Extracts error estimates for retr quantities from covariance\n",
    "##  matrices for the error due to measurement noise covmat_so and the error \n",
    "##  due to limited resolution of the observation system\" -- fwd model and smoothing errors.\n",
    "ws.retrievalErrorsExtract( ws.covmat_so, ws.covmat_ss)  \n",
    "#ws.retrievalErrorsExtract( ws.retrieval_eo, ws.retrieval_ss, ws.covmat_so, ws.covmat_ss) # returns first two \n",
    "print(np.shape(ws.retrieval_eo.value), np.shape(ws.retrieval_ss.value))\n",
    "obs_err = np.copy(ws.retrieval_eo.value[:len_rgrid].reshape(r_lat.size,r_lon.size,order=\"F\"))\n",
    "smo_err = np.copy(ws.retrieval_ss.value[:len_rgrid].reshape(r_lat.size,r_lon.size,order=\"F\"))\n",
    "\n",
    "# some smoothing errors can show up as negative values in covmat_ss diagonal...\n",
    "smo_err[smo_err != smo_err] = 0.0\n",
    "if nrvar>1:\n",
    "    obs_err2 = np.copy(ws.retrieval_eo.value[len_rgrid:].reshape(r_lat.size,r_lon.size,order=\"F\"))\n",
    "    smo_err2 = np.copy(ws.retrieval_ss.value[len_rgrid:].reshape(r_lat.size,r_lon.size,order=\"F\"))\n",
    "    smo_err2[smo_err2 != smo_err2] = 0.0\n",
    "    \n",
    "vmi,vma = .1,.5\n",
    "cm=cmo.thermal\n",
    "#plt.pcolor(obs_err,vmin=vmi,vmax=vma,cmap=cm)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "#plt.pcolor(smo_err,vmin=vmi,vmax=vma,cmap=cm)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "#plt.pcolor(obs_err2,vmin=vmi,vmax=vma,cmap=cm)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "#plt.pcolor(smo_err2,vmin=vmi,vmax=vma,cmap=cm)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "#plt.pcolor(obs_err+smo_err,vmin=.2,vmax=.6,cmap=cm)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "#plt.pcolor(obs_err2+smo_err2,vmin=.2,vmax=.6,cmap=cm)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print( ( ws.retrieval_eo.value ))\n",
    "print( ( smo_err ))\n",
    "print( np.diag(smoov) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save total errr plot\n",
    "f, axs = plt.subplots(1, 2, figsize=[15,5])\n",
    "ax_sst, ax_wsp = axs\n",
    "errno = plt.Normalize(vmin=.0,vmax=1.2)\n",
    "cm=cmo.thermal\n",
    "ax_sst.pcolor(obs_err +smo_err,  norm=errno, cmap=cm)\n",
    "ax_wsp.pcolor(obs_err2+smo_err2, norm=errno, cmap=cm)\n",
    "sm = plt.cm.ScalarMappable(cmap=cm, norm=errno)\n",
    "sm._A = []\n",
    "cb = plt.colorbar(sm,ax=ax_sst,label='SST (tot) error')\n",
    "sm2 = plt.cm.ScalarMappable(cmap=cm, norm=errno)\n",
    "sm2._A = []\n",
    "cb2 = plt.colorbar(sm2,ax=ax_wsp,label='Wind (tot) error')\n",
    "f.savefig('')\n",
    "if sv: plt.savefig('imgr/tot-err_'+pv+'.png',bbox_inches='tight',dpi=350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smo_err.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from middle of retrieval grid, judge where the dominant error is coming from:\n",
    "#print('ratio of smoothing to obs error (sst): ',np.nanmean(smo_err[5:-5,8:-8]/obs_err[5:-5,8:-8]))\n",
    "#print('ratio of smoothing to obs error (wsp): ',np.nanmean(smo_err2[5:-5,8:-8]/obs_err2[5:-5,8:-8]))\n",
    "#andstophere\n",
    "\n",
    "#print( smo_err[5:-5,8:-8])\n",
    "#lt.pcolor(obs_err2+smo_err2,vmin=vmi,vmax=vma)\n",
    "#lt.colorbar()\n",
    "#lt.show()\n",
    "plt.plot(ws.y.value - ws.yf.value)  # quick dump of sim vs. 'obs' TBs in no order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map variables back to scan/pixel arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Amat_d_re = np.zeros_like(x_back) # just diagonal elements from A matrix, back to rlat/rlon grid\n",
    "Amat_d_re[0,:,:] = np.copy( np.diagonal(Amat)[:len_rgrid].reshape(r_lat.size,r_lon.size,order=\"F\"))\n",
    "if nrvar>1:\n",
    "    Amat_d_re[1,:,:] = np.copy( np.diagonal(Amat)[len_rgrid:].reshape(r_lat.size,r_lon.size,order=\"F\"))\n",
    "    \n",
    "# want to fill [npts,rlat,rlon] grid with A values for each retrieval var\n",
    "\n",
    "Amat_chosen1 = np.copy( Amat[:len_rgrid, :len_rgrid].reshape(len_rgrid,r_lat.size,r_lon.size,order=\"F\") )\n",
    "print(np.shape(Amat_chosen1))\n",
    "# first quadrant of Amat (SST)\n",
    "if nrvar>1:\n",
    "    ## 2nd quadrant of Amat (wind)\n",
    "    Amat_chosen2 = np.copy( Amat[len_rgrid:, len_rgrid:].reshape(len_rgrid,r_lat.size,r_lon.size,order=\"F\") )\n",
    "\n",
    "# pick a rgrid point in middle of grid and extract Amat:\n",
    "ehn = int(len_rgrid/2.0)# +32)\n",
    "print(ehn)\n",
    "plt.figure(figsize=[12,9])\n",
    "plt.pcolor(Amat_chosen1[ehn,:,:])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=[12,9])\n",
    "plt.pcolor(Amat_chosen2[ehn,:,:])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "#plt.figure(figsize=[16,12])\n",
    "#plt.pcolor(Amat_d_re[0,:,:])\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "#plt.figure(figsize=[16,12])\n",
    "#plt.pcolor(Amat_d_re[1,:,:])\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take one row of A matrix from each retr variable block:\n",
    "dexes = np.random.choice(range(len_rgrid), 10)\n",
    "for c in dexes:\n",
    "    print('sst @',c,np.sum(Amat[:len_rgrid, :len_rgrid][c,:]))\n",
    "    print('wsp @',c,np.sum(Amat[len_rgrid:, len_rgrid:][c,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like (with all freqs) wind has maybe 12-20km resolution,  more like 20-30km for SST\n",
    "plt.figure()\n",
    "plt.plot(Amat_chosen1[ehn,int(r_lat.size/2),:]) #35:60\n",
    "plt.plot(Amat_chosen2[ehn,int(r_lat.size/2),:])\n",
    "plt.figure()\n",
    "plt.plot(Amat_chosen1[ehn,:,int(r_lon.size/2)]) #10:38\n",
    "plt.plot(Amat_chosen2[ehn,:,int(r_lon.size/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# yf is y_fit whereas y itself is the measurement vector here, so obs-sim is y-yf:\n",
    "ydif = np.copy( ws.y.value - ws.yf.value )\n",
    "ysim = np.copy( ws.yf.value)\n",
    "\n",
    "xretr = np.zeros_like(x_back)\n",
    "xretr[0,:,:] = np.copy(ws.x.value[:len_rgrid].reshape(r_lat.size,r_lon.size,order=\"F\"))\n",
    "if nrvar>1:\n",
    "    xretr[1,:,:] = np.copy(ws.x.value[len_rgrid:].reshape(r_lat.size,r_lon.size,order=\"F\"))\n",
    "\n",
    "xdif = xretr - x_back    # save retr vs prior/background on rgrid\n",
    "xdif_a = xretr - xa       # save difference of apriori vs retrieved\n",
    "\n",
    "geo = np.copy(ws.y_geo.value) #columns are altitude, lat, lon, zenith, azimuth of measurement geoposition\n",
    "print(np.shape(ws.y.value))\n",
    "# y vector should be of size [nf*2 (stokes dim 2) ] * npix*nscans  \n",
    "\n",
    "# vars coming out of yCalc should be in order, nch then npix then nscans, i.e.:\n",
    "## with nch=6 first 6 values are from pix1,sc1 then next 6 are pix2,sc1 and so on -- be careful!\n",
    "# can compare geo[:,3] (az) to EIA from L1R (later), just 180-az to get EIA\n",
    "\n",
    "\n",
    "# convert vectors back to scan/pixel matrices:\n",
    "sim_tb = np.zeros([nscans,npix,nf*2])\n",
    "dif_tb = np.zeros([nscans,npix,nf*2]) \n",
    "arts_pos = np.zeros([nscans,npix,4]) \n",
    "print(nf*2,npix,nscans)\n",
    "\n",
    "for c in range(nf*2):\n",
    "    sim_tb[:,:,c] = ysim[c::nf*2].reshape(nscans,npix)\n",
    "    dif_tb[:,:,c] = ydif[c::nf*2].reshape(nscans,npix)\n",
    "    for bb in range(4):\n",
    "        arts_pos[:,:,bb] = geo[c::nf*2,bb+1].reshape(nscans,npix)  # lat,lon,zenith,azimuth\n",
    "        ## arts obs points -- can differ slightly from prescribed, defined where max response is registered!\n",
    "        \n",
    "    \n",
    "print(np.shape(dif_tb))\n",
    "plt.imshow(dif_tb[:,:,0]) # 6V obs-sim\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "#plt.imshow(dif_tb[:,:,nf*2-1]) # last _H obs-sim\n",
    "#plt.colorbar()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(geo.shape, arts_pos.shape)\n",
    "#for i in range(4):\n",
    "#    print(info(arts_pos[:,:,i]))\n",
    "#plt.pcolor(arts_pos[:,:,3])\n",
    "#plt.colorbar()\n",
    "# as a check on the angular setup, make sure that y_geo from ARTS (position of each boresight)\n",
    "#  matches (almost exactly) the L1R lat/lon values:\n",
    "#print(info(arts_pos[:,:,0]), info(ala[pxt:pxp,sct:scp]))\n",
    "#print(info(arts_pos[:,:,1]), info(alo[pxt:pxp,sct:scp]))\n",
    "print(info(arts_pos[:,:,1]-alo[pxt:pxp,sct:scp].transpose()))\n",
    "print(info(arts_pos[:,:,0]-ala[pxt:pxp,sct:scp].transpose())) # stats on location differences\n",
    "\n",
    "#  shows difference in L1 vs. ARTS longitude from middle, first, and last scans run:\n",
    "plt.plot( alo[pxt:pxp,scm], alo[pxt:pxp,scm]-arts_pos[int(nscans/2),:,1],label='mid')\n",
    "plt.plot( alo[pxt:pxp,sct], alo[pxt:pxp,sct]-arts_pos[0,:,1],label='first')\n",
    "plt.plot( alo[pxt:pxp,scp], alo[pxt:pxp,scp]-arts_pos[-1,:,1],label='last')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting representative ellipses:\n",
    "from matplotlib.patches import Ellipse\n",
    "from cartopy import crs as ccrs\n",
    "from cmocean import cm as cmo\n",
    "# first define widths/heights of ellipses that represent HPBW FOV for each freq:\n",
    "widths  = np.array([35., 35., 24., 14., 15.,  7., 3.])[fsub] # in km across\n",
    "heights = np.array([62., 62., 42., 22., 26., 12., 5.])[fsub] #  per freq, 6/7/10/18/23/36/89\n",
    "#if los0[0] != 132.5:\n",
    "#    print('ELLIPSE SIZES INCORRECT DUE TO non-AMSR sensor angle')\n",
    "\n",
    "# convert (roughly!) to deg near equator... okay as first approximation for plotting...\n",
    "#  if plotting near poles then will need to be more rigorous.\n",
    "widths *= 360.0/40075. / np.abs(np.cos(np.mean(ri))) # scale widths by cos(lat)\n",
    "print(widths)\n",
    "heights*= 360.0/40075. # deg/circumference\n",
    "\n",
    "\n",
    "#print('VERIFY THAT THESE ARE WHOLLY EVEN ON THE GRID!')\n",
    "#print(np.diff(arts_pos[0,:,1]))\n",
    "#print(np.diff(arts_pos[:,0,0]))\n",
    "\n",
    "projj = ccrs.PlateCarree()\n",
    "inn =  0.5\n",
    "reg = [np.min(la_grid)+inn, np.max(la_grid)-inn,\n",
    "           np.min(lo_grid)+inn, np.max(lo_grid)-inn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ks = [0,5,nf*2-1] # channel indices to plot\n",
    "\n",
    "for k in range(nf*2): #ks:\n",
    "    # provide x,y coords, width, height, angle of each FOV-- make an ellipse\n",
    "    ells = [Ellipse(xy=[arts_pos[i,j,1],arts_pos[i,j,0]], width=widths[int(k/2)], height=heights[int(k/2)], \n",
    "                    angle=arts_pos[i,j,3], linewidth=0.2, alpha=0.7, fill=False) \n",
    "            for i in range(nscans) for j in range(npix)]# for k in [0]] #range(nf*2)]\n",
    "\n",
    "    fig = plt.figure(figsize=[14,8])\n",
    "    ax = plt.subplot(111, projection=projj )\n",
    "\n",
    "    ax.set_xlim(left=reg[2], right=reg[3])\n",
    "    ax.set_ylim(bottom=reg[0], top=reg[1])\n",
    "    nx,ny = 5,5\n",
    "    ax.set_xticks( np.linspace(reg[2],reg[3], nx) )\n",
    "    ax.set_yticks( np.linspace(reg[0],reg[1], ny) )\n",
    "\n",
    "    #sdif = np.max(np.abs(tmean-s_data[0,:,:])) # set colorbar limits by largest abs deviation from tmean?\n",
    "    #thenorm1=plt.Normalize(vmin = np.min(s_data[0,:,:])-1, vmax = np.max(s_data[0,:,:])+1)\n",
    "    topl = 1\n",
    "    thenorm1=plt.Normalize(vmin = np.min(xretr[topl,:,:]), vmax = np.max(xretr[topl,:,:]))\n",
    "    ceem1=cmo.dense\n",
    "    #bb= ax.pcolormesh(lon_grid,lat_grid,s_data[0,:,:],norm=thenorm1, # so SST\n",
    "    bb= ax.contourf(ws.lon_grid,ws.lat_grid,s_data[topl,:,:], 30, norm=thenorm1, # so SST\n",
    "                 transform=projj, cmap=ceem1)\n",
    "    ax.coastlines(resolution='10m')\n",
    "    for e in ells[:]: \n",
    "        ax.add_artist(e)\n",
    "\n",
    "    vmin, vmax = -np.max(np.abs(dif_tb[:,:,k])), np.max(np.abs(dif_tb[:,:,k]))\n",
    "    thenorm = plt.Normalize(vmin=vmin,vmax=vmax)\n",
    "    ceem = cmo.balance #thermal\n",
    "    ax.scatter( arts_pos[:,:,1], arts_pos[:,:,0], s=100.0, c=dif_tb[:,:,k], marker='o', \\\n",
    "                transform=projj, alpha=0.8, cmap=ceem, norm=thenorm);\n",
    "    sm = plt.cm.ScalarMappable(cmap=ceem, norm=thenorm)\n",
    "    sm._A = []\n",
    "    cb = plt.colorbar(sm,ax=ax)\n",
    "    cb.set_label(ch_str[k]+' TB [K] (obs-sim)')\n",
    "    sm1 = plt.cm.ScalarMappable(cmap=ceem1, norm=thenorm1)\n",
    "    sm1._A = []\n",
    "    cb1 = plt.colorbar(sm1,ax=ax)\n",
    "    if topl==0: cb1.set_label('SST [K]')\n",
    "    if topl==1: cb1.set_label('Wind [m/s]')\n",
    "\n",
    "    if sv: plt.savefig('imgr/retr'+str(topl)+'-y_syn_'+ch_str[k]+'_'+pv+'.png',bbox_inches='tight',dpi=350)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# and for the state vector, retrieved vs. a priori laid over background:\n",
    "pvar = xdif # choose what to  plot here\n",
    "\n",
    "for rv in range(nrvar): \n",
    "    if rv==0: tit='SST [K]'\n",
    "    if rv==1: tit='Wind speed [$m s^{-1}$]'\n",
    "    fig2 = plt.figure(figsize=[18,10])\n",
    "    ax2 = plt.subplot(111, projection=projj )\n",
    "\n",
    "    ax2.set_xlim(left=reg[2], right=reg[3])\n",
    "    ax2.set_ylim(bottom=reg[0], top=reg[1])\n",
    "    ax2.set_xticks( np.linspace(reg[2],reg[3], nx) )\n",
    "    ax2.set_yticks( np.linspace(reg[0],reg[1], ny) )\n",
    "    ax2.set_ylabel( 'Latitude [$^o N$]' )\n",
    "    ax2.set_xlabel( 'Longitude [$^o E$]' )\n",
    "\n",
    "    thenorm1=plt.Normalize(vmin = np.min(s_data[rv,:,:])-1, vmax = np.max(s_data[rv,:,:])+1)\n",
    "    #bb= ax2.pcolormesh(lon_grid,lat_grid,s_data[0,:,:],norm=thenorm1, \n",
    "    bb= ax2.contourf(ws.lon_grid,ws.lat_grid,s_data[rv,:,:], 10, norm=thenorm1, \n",
    "                 transform=projj, cmap=ceem1)\n",
    "\n",
    "    #for e in ells[:]: \n",
    "    #    ax2.add_artist(e)\n",
    "\n",
    "    vmin, vmax = -np.max(np.abs(pvar[rv,:,:])), np.max(np.abs(pvar[rv,:,:]))\n",
    "    thenorm = plt.Normalize(vmin=vmin,vmax=vmax)\n",
    "    ceem = cmo.balance #thermal\n",
    "    # ri,rj defined earlier via meshgrid, i.e. retrieval lon and lat grids respectively\n",
    "    ax2.scatter( rj, ri, s=100.0, c=pvar[rv,:,:], marker='o', \\\n",
    "                transform=projj, alpha=0.8, cmap=ceem, norm=thenorm);\n",
    "    sm = plt.cm.ScalarMappable(cmap=ceem, norm=thenorm)\n",
    "    sm._A = []\n",
    "    cb = plt.colorbar(sm,ax=ax2)\n",
    "    cb.set_label(tit+' (retr - era5)') #prior)')\n",
    "    sm1 = plt.cm.ScalarMappable(cmap=ceem1, norm=thenorm1)\n",
    "    sm1._A = []\n",
    "    cb1 = plt.colorbar(sm1,ax=ax2)\n",
    "    cb1.set_label(tit)\n",
    "\n",
    "    if sv: plt.savefig('imgr/retr-x_syn_rv'+str(rv)+'_'+pv+'.png',bbox_inches='tight',dpi=350)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK: plot background (synthetic defined) state vs. ERA5 on retrieval grid. \n",
    "#    should be simply interpolated values!\n",
    "\n",
    "# and for the state vector, retrieved vs. a priori laid over background:\n",
    "projj = ccrs.PlateCarree()\n",
    "inn =  0.6\n",
    "nx,ny = 5,5\n",
    "for rv in range(nrvar): \n",
    "    if rv==0: tit='SST [K]'\n",
    "    if rv==1: tit='Wind speed [$m s^{-1}$]'\n",
    "    fig2 = plt.figure(figsize=[18,10])\n",
    "    ax2 = plt.subplot(111, projection=projj )\n",
    "\n",
    "    ax2.set_xlim(left=reg[2], right=reg[3])\n",
    "    ax2.set_ylim(bottom=reg[0], top=reg[1])\n",
    "    ax2.set_xticks( np.linspace(reg[2],reg[3], nx) )\n",
    "    ax2.set_yticks( np.linspace(reg[0],reg[1], ny) )\n",
    "    ax2.set_ylabel( 'Latitude [$^o N$]' )\n",
    "    ax2.set_xlabel( 'Longitude [$^o E$]' )\n",
    "\n",
    "    ceem = cmo.thermal\n",
    "    #thenorm1=plt.Normalize(vmin = np.min(s_data[rv,:,:])-3, vmax = np.max(s_data[rv,:,:])+3)\n",
    "    thenorm1=plt.Normalize(vmin = np.min(xretr[rv,:,:]), vmax = np.max(xretr[rv,:,:]))\n",
    "    #bb= ax2.pcolormesh(lon_grid,lat_grid,s_data[0,:,:],norm=thenorm1, \n",
    "    #bb= ax2.contourf(ws.lon_grid,ws.lat_grid,s_data[rv,:,:], 10, norm=thenorm1, \n",
    "    bb= ax2.scatter(gj, gi, s=300, c=s_data[rv,:,:], norm=thenorm1, \n",
    "                 transform=projj, cmap=ceem)\n",
    "\n",
    "    #for e in ells[:]: \n",
    "    #    ax2.add_artist(e)\n",
    "\n",
    "    #vmin, vmax = -np.max(np.abs(xdif2[rv,:,:])), np.max(np.abs(xdif2[rv,:,:]))\n",
    "    #thenorm = plt.Normalize(vmin=vmin,vmax=vmax)\n",
    "    # ri,rj defined earlier via meshgrid, i.e. retrieval lon and lat grids respectively\n",
    "    ax2.scatter( rj, ri, s=80.0, c=xretr[rv,:,:], marker='o', \\\n",
    "                transform=projj, alpha=0.8, cmap=ceem, norm=thenorm1);\n",
    "    #sm = plt.cm.ScalarMappable(cmap=ceem, norm=thenorm1)\n",
    "    #sm._A = []\n",
    "    #cb = plt.colorbar(sm,ax=ax2)\n",
    "    #cb.set_label(tit+' (retr - prior)')\n",
    "    sm1 = plt.cm.ScalarMappable(cmap=ceem, norm=thenorm1)\n",
    "    sm1._A = []\n",
    "    cb1 = plt.colorbar(sm1,ax=ax2)\n",
    "    cb1.set_label(tit)\n",
    "    ax2.set_title(\"Retr grid over background (ERA5)\")\n",
    "\n",
    "    #if sv: plt.savefig('imgr/retr-x_syn_rv'+str(rv)+'_'+pv+'.png',bbox_inches='tight',dpi=350)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stapppp # who cares (for now)\n",
    "#\n",
    "## and for the DOF, take elements from A matrix diagonal:\n",
    "#fig3 = plt.figure(figsize=[15,10])\n",
    "#ax3 = plt.subplot(111, projection=projj )\n",
    "#\n",
    "#ax3.set_xlim(left=reg[2], right=reg[3])\n",
    "#ax3.set_ylim(bottom=reg[0], top=reg[1])\n",
    "#ax3.set_xticks( np.linspace(reg[2],reg[3], nx) )\n",
    "#ax3.set_yticks( np.linspace(reg[0],reg[1], ny) )\n",
    "#\n",
    "#bb= ax3.pcolormesh(lon_grid,lat_grid,s_data[0,:,:],norm=thenorm1, # so SST\n",
    "#             transform=projj, cmap=ceem1)\n",
    "#\n",
    "##for e in ells[:]: \n",
    "##    ax3.add_artist(e)\n",
    "#    \n",
    "#vmin, vmax = 0, 1 #-np.max(np.abs(xdif[:,:])), np.max(np.abs(xdif[:,:]))\n",
    "#thenorm = plt.Normalize(vmin=vmin,vmax=vmax)\n",
    "#ceem = cmo.solar\n",
    "## ri,rj defined earlier via meshgrid\n",
    "#ax3.scatter( ri, rj, s=200.0, c=Amat_re[0,:,:], marker='o',    # pick which Amat vars?\n",
    "#            transform=projj, alpha=0.8, cmap=ceem, norm=thenorm);\n",
    "#sm = plt.cm.ScalarMappable(cmap=ceem, norm=thenorm)\n",
    "#sm._A = []\n",
    "#cb = plt.colorbar(sm,ax=ax3)\n",
    "#cb.set_label('DOFS')\n",
    "#sm1 = plt.cm.ScalarMappable(cmap=ceem1, norm=thenorm1)\n",
    "#sm1._A = []\n",
    "#cb1 = plt.colorbar(sm1,ax=ax3)\n",
    "#cb1.set_label('SST [K]')\n",
    "#\n",
    "##plt.savefig('imgr/retr-A_syn_'+pv+'.png',bbox_inches='tight',dpi=350)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to run fastem alone for testing, outputting emissivities and expected TBs with no atmosphere:\n",
    "#ws.MatrixCreate(\"emissivity\")\n",
    "#ws.MatrixCreate(\"reflectivity\")\n",
    "#ws.reflectivity = np.zeros([f_grid.size,4])\n",
    "#ws.emissivity   = np.zeros([f_grid.size,4])\n",
    "##ws.surface_skin_t = tmean #s_data[0,:,:]\n",
    "##ws.transmittance\n",
    "#ws.transmittance = np.ones( ws.f_grid.value.shape ) * 0.7\n",
    "#yep = 34\n",
    "#emii = np.zeros([yep,nf,2])\n",
    "#tbout= np.zeros([yep,nf,2])\n",
    "#tmean=270.0\n",
    "#for y in range(yep):\n",
    "#    ws.surface_skin_t = tmean+float(y) #s_data[0,:,:]\n",
    "#    ws.FastemStandAlone(surface_skin_t= tmean+y, f_grid=ws.f_grid,\n",
    "#                        emissivity=ws.emissivity, reflectivity=ws.reflectivity,\n",
    "#                        za=125.0, wind_speed=wsp, rel_aa=0.0, transmittance=ws.transmittance.value)\n",
    "#    emii[y,:,:] = ws.emissivity.value[:,0:2]\n",
    "#    tbout[y,:,:]= ws.emissivity.value[:,0:2]*tmean+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### soooo, emissivity varies quite little with temperature for 6V (0.557->.553->.56),\n",
    "#     declines slightly at 18 (.64->.58), then bigtime at 89 (.84->.74)\n",
    "##print(ws.emissivity.value)\n",
    "#print(emii[:,0,0]) # 6\n",
    "##print(emii[:,3,0]) # 18\n",
    "##print(emii[:,5,0]) # 36\n",
    "#print(emii[:,-1,0]) # 89\n",
    "#\n",
    "#print(tbout[:,1,0])\n",
    "##print(tbout[:,2,0])\n",
    "#print(tbout[:,-1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(ws.covmat_sx.from_typhon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
