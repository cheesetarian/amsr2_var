{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- ARTS multi-FOV 2DVAR surface retrievals --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  David Duncan, Chalmers University, Feb 2019\n",
    "\n",
    "#  this script uses ARTS OEM to experiment with retrieval of a synthetic scene. \n",
    "#    the goal here is to probe the spatial resolution of geophysical parameter \n",
    "#    that may be retrievable with such a sensor, focused on AMSR2 and surface params\n",
    "\n",
    "# this setup, in steps/words:\n",
    "### - define an antenna response that is frequency dependent, gaussian, symmetric\n",
    "### - define sensor position and line of sight for each scan at middle of scan\n",
    "### - define an angular grid (zenith & azimuth) for each scan that encompasses npix\n",
    "### - sensor angles are all 'absolute' whereas the angular grid and antenna response \n",
    "###    are all relative to the central bore sight of the scan\n",
    "### - a lat/lon grid is also defined that must encompass all observation points simulated\n",
    "### - the same angular grid and simulation setup is assumed/copied for all nscans, and \n",
    "###    mblock_dlos_grid allows all simulations to be run at the same time\n",
    "\n",
    "### *** this setup outputs simulates TBs for chosen channels of AMSR2\n",
    "###    with antenna patterns taken into account and decoupled from any retrieval grid, with\n",
    "###    pencil beam calculations sampling the antenna pattern according to the angular grid\n",
    "###    defined\n",
    "\n",
    "### a priori covariances are defined in terms of standard deviations and decorrelation lengths\n",
    "###  and observation error covariances are frequency dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define # of vars, channels, size of retrieval, and covariance assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deco_sst = 0.5  # SST decorrelation length of a priori [deg] -- set to near-zero if wanting none\n",
    "deco_wsp = 0.8  # wind decorrelation length of a priori [deg] \n",
    "pv = 'v4' # plot version (end of plots' filenames)\n",
    "sv = True     # whether or not to save output plots\n",
    "\n",
    "# frequency subset, defined by AMSR2 frequency indices: 0=6.9GHz, 1=7.3, 2=10.6, 3=18.7, 4=23.8, 5=36.5, 6=89\n",
    "#fsub  = np.array([0, 1, 2]) # choose which frequencies to run (runs both polarisations--v/h)\n",
    "fsub = np.array([0,1,2,3])#,4,5,6])\n",
    "\n",
    "nrvar    = 2     # number of retrieval variables, 1 will be SST only, 2 for SST+windspeed\n",
    "\n",
    "npix     =  25   # pixels across each scan considered\n",
    "nscans   =  25   # consecutive scans considered\n",
    "angfac   =   3   # angular samples per degree or between pix -- should be min ~10 (but depends on freqs used)\n",
    "sx_sst   = 0.8   # a priori std deviation for SST\n",
    "sx_wsp   = 1.8   # a priori std deviation for wind speed (if nrvar > 1)\n",
    "xcorr    = 0.0   # cross correlation between SST and wind speed\n",
    "\n",
    "# these should likely remain fixed:\n",
    "nedt   = np.array([0.34, 0.43, 0.7, 0.7, 0.6, 0.7, 1.2])[fsub] # from published NEdT values\n",
    "npol     =  2    # number of polarizations considered (2=V/H both, 1=intensity only)\n",
    "\n",
    "#  note: for speed, the angular grid is the biggest limiting factor, so if using low frequencies only\n",
    "#    then angfac<~5 is quite justified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid resolution is  0.2 0.2\n"
     ]
    }
   ],
   "source": [
    "# Define lat/lon grid that observation points and angular grid lie within\n",
    "lat0     = -3.0  # center of grid in lat, lon \n",
    "lon0     = 4.0\n",
    "nlat, nlon = 20, 25        # number of lat and lon divisions\n",
    "latwid, lonwid = 2.0, 2.5  # actual grid widths are double these\n",
    "\n",
    "resa = 2*latwid/nlat  #lat grid resolution\n",
    "reso = 2*lonwid/nlon  #lon grid resolution\n",
    "print('Grid resolution is ', resa, reso)\n",
    " \n",
    "lat_grid = lat0 + np.arange(-latwid, latwid+resa, resa)\n",
    "lon_grid = lon0 + np.arange(-lonwid, lonwid+reso, reso)\n",
    "# keep these (non-ARTS vars) for sake of plotting at the end!\n",
    " \n",
    "#print('corners of lat_grid: ',lat_grid)\n",
    "#print('midpoints of lat_grid (ARTS ws grid): ',lat_grid[0:nlat]+np.diff(lat_grid)*.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up ARTS environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ARTS_INCLUDE_PATH=/home/dudavid/arts/controlfiles/\n",
      "env: ARTS_BUILD_PATH=/home/dudavid/arts/build/\n",
      "env: ARTS_DATA_PATH=/home/dudavid/arts/arts-xml/\n",
      "Loading ARTS API from: /home/dudavid/arts/build/src/libarts_api.so\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%env ARTS_INCLUDE_PATH=/home/dudavid/arts/controlfiles/\n",
    "%env ARTS_BUILD_PATH=/home/dudavid/arts/build/\n",
    "%env ARTS_DATA_PATH=/home/dudavid/arts/arts-xml/ \n",
    "\n",
    "%matplotlib inline\n",
    "#from h5py import File\n",
    "from scipy.spatial.distance import pdist      # calculate pair-wise distances (euclidean)\n",
    "from scipy.spatial.distance import squareform # convert from reduced matrix to square\n",
    "\n",
    "from typhon.arts.workspace import Workspace, arts_agenda\n",
    "ws = Workspace(verbosity=0)\n",
    "ws.execute_controlfile(\"general/general.arts\")\n",
    "ws.execute_controlfile(\"general/continua.arts\")\n",
    "ws.execute_controlfile(\"general/agendas.arts\")\n",
    "ws.execute_controlfile(\"general/planet_earth.arts\")\n",
    "\n",
    "from typhon.arts.workspace.variables import *\n",
    "\n",
    "# set various ARTS agendas:\n",
    "ws.Copy( ws.abs_xsec_agenda, ws.abs_xsec_agenda__noCIA )\n",
    "ws.Copy( ws.iy_main_agenda, ws.iy_main_agenda__Emission )\n",
    "ws.Copy( ws.iy_space_agenda, ws.iy_space_agenda__CosmicBackground )\n",
    "ws.Copy( ws.propmat_clearsky_agenda, ws.propmat_clearsky_agenda__OnTheFly )\n",
    "ws.Copy( ws.ppath_agenda, ws.ppath_agenda__FollowSensorLosPath )\n",
    "ws.Copy( ws.ppath_step_agenda, ws.ppath_step_agenda__GeometricPath )\n",
    "@arts_agenda\n",
    "def geo_pos_agendaPY(ws):\n",
    "    ws.geo_posEndOfPpath()\n",
    "ws.Copy( ws.geo_pos_agenda, geo_pos_agendaPY)  ## new one, set by patrick in his cfile\n",
    "\n",
    "# define absorbing species and sensor (here using metmm library, used again below)\n",
    "ws.abs_speciesSet(species=[\"H2O-PWR98\",\"O2-PWR93\",\"N2-SelfContStandardType\"])#,\"liquidcloud-ELL07\"])\n",
    "ws.abs_lines_per_speciesSetEmpty()\n",
    "\n",
    "ws.stokes_dim = npol     # to get V and H pol set to 2\n",
    "ws.iy_unit = \"PlanckBT\"  # equivalent: ws.StringSet( iy_unit, \"PlanckBT\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WorkspaceVariable' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ca9f71426132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAtmosphereSet3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlat_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnlat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m.5\u001b[0m \u001b[0;31m# to avoid mismatch between ARTS and pcolormesh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlon_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlon_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnlon\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlon_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m.5\u001b[0m \u001b[0;31m#  i.e. getting lat/lon midpoints of boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'WorkspaceVariable' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# set up atmosphere and surface variables:\n",
    "ws.atmosphere_dim = 3  # 3D atmosphere\n",
    "p = np.array([1015.,950.,800.])*100.0 #keep it simple, otherwise slower fwd model -- IGNORES LOTS OF WV!!!\n",
    "#p = np.array([1015.,975.,950.,925.,900.,850.,800.,750.,700.,650.,600.,550.,500.,400.,300.,200.,100.])*100.0\n",
    "ws.p_grid = p[:] \n",
    "ws.AtmRawRead( basename = \"planets/Earth/Fascod/tropical/tropical\") #tropical atmosphere assumed\n",
    "ws.AtmosphereSet3D()\n",
    "\n",
    "ws.lat_grid = lat_grid[0:nlat] + np.diff(lat_grid)*.5 # to avoid mismatch between ARTS and pcolormesh\n",
    "ws.lon_grid = lon_grid[0:nlon] + np.diff(lon_grid)*.5 #  i.e. getting lat/lon midpoints of boxes\n",
    "\n",
    "ws.AtmFieldsCalcExpand1D()  # set to given p_grid or z_grid\n",
    "\n",
    "#ws.vmr_field.value[0,:,:,:] *= 0.50 # try decreasing water vapor by X\n",
    "\n",
    "# if using coarse atmosphere but wanting accurate ray tracing:\n",
    "ws.ppath_lmax = 350.0  # set maximum distance between points when computing absorption along path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define synthetic ocean surface scene, set in ARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Surface properties -- define ocean surface for simulations\n",
    "\n",
    "nala, nalo = np.size(ws.lat_grid.value), np.size(ws.lon_grid.value) # size of ARTS lat/lon grid\n",
    "s_names = [\"Water skin temperature\",\"Wind speed\",\"Wind direction\",\"Salinity\"]\n",
    "s_data = np.zeros([len(s_names), nala, nalo])\n",
    "\n",
    "tmean                   = 282.0 # sst mean\n",
    "wsp                     = 7.3   # 10m wind speed\n",
    "s_data[0,:,:] = tmean\n",
    "#s_data[0,int(nlat/2)-1:int(nlat/2)+2,int(nlon/2)-1:int(nlon/2)+2] += 6\n",
    "#s_data[0,:,::8] += 0.9  ## czechers\n",
    "s_data[0,int(nlat/2)-2:int(nlat/2)+3,int(nlon/2)-2:int(nlon/2)+3] += 5.0\n",
    "s_data[0,:,:] += np.random.normal(0, 2, [nala,nalo])\n",
    "s_data[0,int(nlat/3)-0:int(nlat/3)+3,:] += 3.0\n",
    "s_data[0,int(nlat*2/3)-3:int(nlat*2/3)+0,:] -= 3.0\n",
    "#s_data[0,int(nlat/3)-1:int(nlat/3)+2,:] += 5.0\n",
    "#s_data[0,:,int(nlon/3)-0:int(nlon/3)+3] += 5.0\n",
    "#s_data[0,:,int(nlon*2/3)-3:int(nlon*2/3)+0] -= 5.0\n",
    "#s_data[0,1::4,::4] -= 2.9  ## czechers\n",
    "#s_data[0,::2,1::3] -= 1.9\n",
    "#s_data[0,::2,2::3] += 0.9\n",
    "#s_data[0,:,:] += np.tile(np.arange(0,nala),[nalo,1])\n",
    "#s_data[0,18:21,18:21] = tmean +6.5 # some preturbation\n",
    "#s_data[0,int(nlat/2),int(nlon/2)] = tmean +9.5 # some preturbation\n",
    "s_data[1,:,:] = wsp\n",
    "#s_data[1,:,:] += np.random.normal(1.2, 1.9, [nala,nalo])\n",
    "s_data[1,:,:] += 0.15 * np.tile(np.arange(0,nala),[nalo,1]).transpose() \n",
    "s_data[2,:,:] = 90 # token wind direction value\n",
    "s_data[3,:,:] = 0.034 # token salinity value\n",
    "\n",
    "# write these to ARTS variables \n",
    "ws.Copy(ws.surface_props_names, s_names)\n",
    "ws.Copy(ws.surface_props_data, s_data)\n",
    "ws.MatrixSetConstant(ws.z_surface, nala, nalo, 0.0) # explicitly set the surface to 0m altitude\n",
    "\n",
    "plt.imshow(s_data[0,:,:]) # quick look for SST\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(s_data[1,:,:]) # quick look for wsp\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define antenna pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set and calculate some basic variables for antenna and scanning\n",
    "\n",
    "### All will be replaced or superseded if using L1R data as input\n",
    " \n",
    "zsat   = 699.7e3      # Satellite altitude [m] -- can read this in later\n",
    "vsat   = 6.76e3       # Satellite velocity [m/s]\n",
    "dt     = 2.6e-3       # Integration time [s] -- 2.6ms for low freqs, 1.3ms for 89GHz\n",
    "rpm    = 40           # Rotations per minute -- same for AMSR-E and AMSR2\n",
    " \n",
    "# nautical mile constant?  1852m = 1nmi\n",
    "m2deg  = 1/(60*1852)                          # Conversion from m to latitude\n",
    "dang   = dt * 360 * rpm / 60                  # Angular distance between samples\n",
    "dlat   = m2deg * vsat * 60 / rpm              # Latitude distance between scans\n",
    "print(dang,dlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "width  = 1.8           # Max half-width of antenna simulated (if HPBW is 1.8deg, width=2 covers 2x the HPBW)\n",
    "resol  = 0.01          # angular resolution (same in zenith/azimuth)\n",
    "# note: for a given angular resolution (might vary), width=2 yields 99.96% of total power @6GHz \n",
    "#   compared to width=20 and 99.99999% @10GHz, width=1.5 yields 99.0 and 99.994 @ 6,10\n",
    "#resol, width = dang/(angfac*5), awidth # set here instead of calling func\n",
    "print('angular resolution of response grid [deg]: ',resol)\n",
    "\n",
    "adata = [[ 6.925e9 ,  1.80],   # AMSR2 center frequency and beamwidth (deg)\n",
    "         [ 7.300e9 ,  1.80],   # assumed the same for V/H polarizations\n",
    "         [ 10.65e9 ,  1.20],\n",
    "         [ 18.70e9 ,  0.65],\n",
    "         [ 23.80e9 ,  0.75],\n",
    "         [ 36.50e9 ,  0.35],\n",
    "         [ 89.00e9 ,  0.15]] \n",
    "\n",
    "adata = np.array(adata, order=\"C\").transpose()[:,fsub] # choose selected frequencies (set above)\n",
    "\n",
    "# define zenith, azimuth grid on the ground (relative to bore sight) -- assumed to be square, 2xWidth wide\n",
    "x  = np.arange( -width, width+resol, resol )  #this gives angular antenna response points in za,aa\n",
    "x2 = x**2   # since assumed origin is 0, do squaring here \n",
    "nf, nx = len(adata[0,:]), np.size(x)   # num frequencies, size of antenna grid\n",
    "\n",
    "csub = np.sort(np.append(fsub*2,fsub*2+1))\n",
    "ch_str = np.array(['6V','6H','7V','7H','10V','10H','18V','18H',\n",
    "                   '23V','23H','36V','36H','89V','89H'])[csub] \n",
    "#print(ch_str)\n",
    "\n",
    "from typhon.arts.griddedfield import GriddedField4\n",
    "gf4 = GriddedField4()   # ARTS variable type, found in typhon\n",
    "gf4.name = 'AMSR2 antenna response'\n",
    "gf4.gridnames =  [ 'Polarisation', 'Frequency', 'Zenith angle', 'Azimuth angle' ]\n",
    "###  note: za & aa are equally spaced, and we're treating V/H as having identical responses\n",
    "if npol==2: gsp=\"1\" \n",
    "else: gsp=\"0\"\n",
    "gf4.grids     = [ [gsp], adata[0,:], x, x ]\n",
    "gf4.dataname  = 'Response'\n",
    "gf4.data      = np.zeros([ 1, nf, nx, nx ], order=\"C\")\n",
    "print('size of antenna_reponse grid: ',gf4.data.shape)\n",
    "\n",
    "for i in range(nf):\n",
    "    si = adata[1,i] / (2*np.sqrt(2*np.log(2)))  # calculate standard deviation first, based on HPBW\n",
    "    gf4.data[0,i,:,:] = np.exp( - np.tile(x2,[nx,1])/si**2 - np.tile(x2,[nx,1]).transpose()/ si**2 )\n",
    "    #gf4.data[0,i,:,:] = np.exp( - (np.tile(x2,[nx,1]) + np.tile(x2,[nx,1]).transpose())/ si**2 ) # equivalent\n",
    "\n",
    "\n",
    "# frequency grid of simulation is defined according to sensor setup above (may change with use of metmm)\n",
    "f_grid = np.copy(gf4.grids[1])  # array with each frequency (not channel)\n",
    "\n",
    "ws.f_grid.value = f_grid\n",
    "print('f_grid: ',ws.f_grid.value)\n",
    " \n",
    "#plt.pcolormesh(gf4.data[0,0,:,:])# to plot antenna pattern of one freq\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define bore sights for one scan, angular grid for fwd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Determine bore-sight angles to use for one scan \n",
    " \n",
    "# psteps defines the pixels across one scan\n",
    "psteps   = np.arange(-(npix-1)/2, (npix-1)/2 +1, 1)  #  if npix is odd then middle boresight is in the middle\n",
    "# ssteps defines position of spacecraft for each scan, evenly distributed around the middle scan\n",
    "ssteps   = np.arange(-(nscans-1)/2, (nscans-1)/2 +1, 1) \n",
    "\n",
    "#  center line of sight... defined as AMSR2 EIA and 0 azimuth angle\n",
    "los0    = [ 180-47.5, 0 ]  # 47.5 is off-nadir angle of AMSR2, 180 for looking straight down\n",
    "\n",
    "# bsights defines the bore sight zenith and azimuth angles across the scan \n",
    "##-- just for one scan, assumed to be same / repeated for all scans\n",
    "bsights = np.array([ np.repeat(los0[0],npix), los0[1]+dang*psteps ]).transpose()  # size: [npix,2]\n",
    "print('bsights:',(bsights[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define angular grid for pencilbeam calculations\n",
    "\n",
    "# can use 'dang' or not, but benefit of using it is having RT calc at each boresight (i.e. antenna beam max)\n",
    "#  in azimuth direction. if using 'dang' for zenith too then it's an evenly spaced angular grid.\n",
    "\n",
    "n = np.floor( width * angfac )  # angular width * angular samples per deg\n",
    "print(n)\n",
    "za_grid = np.array( los0[0] +  np.arange(-n,n+1)/angfac)  ## effectively taking 'width' on either side of 0\n",
    "#n = np.floor( angfac * width / dang )  # number per degree * angular width / distance between boresights\n",
    "#za_grid = np.array( los0[0] + (dang/angfac) * np.arange(-n,n+1))  # zenith angle grid\n",
    "\n",
    "en = np.floor( angfac * ( bsights[-1,1] + width ) / dang ) # azimuth of boresight at edge +width\n",
    "print(en)\n",
    "aa_grid = np.array( los0[1] + (dang/angfac) * np.arange(-en,en+1))  # azimuth angle grid\n",
    "#en = np.floor((bsights[-1,1]+width)*angfac) ## yields az from boresight at edge plus 'width' (could prob do width/2)\n",
    "#aa_grid = np.array( los0[1] + np.arange(-en,en+1/angfac)/angfac)  # azimuth angle grid\n",
    "\n",
    "print('zenith grid: ',za_grid) \n",
    "print('azimuth grid: ',aa_grid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sensor_pos (position), sensor_los (line of sight) for ARTS\n",
    "\n",
    "ws.sensor_los = np.tile(los0, [nscans,1])  # sensor line of sight for each scan\n",
    "\n",
    "shift   = 7.3   # Latitude shift to centre calculations around lat0\n",
    "\n",
    "# sensor_pos should be columns of altitude, SClat, SClon  ###  \n",
    "#   with real data these come from L1R files and transform X/Y/Z to lat/lon\n",
    "#   currently one alt/sclat/sclon for each scan (i.e. each of nscans)\n",
    "\n",
    "ws.sensor_pos = np.hstack([np.repeat(zsat, nscans).reshape(nscans,1),\n",
    "                          np.array(lat0 - shift + dlat * ssteps).reshape(nscans,1), \n",
    "                          np.repeat(lon0, nscans).reshape(nscans,1)])\n",
    "\n",
    "#print(ws.sensor_pos.value)\n",
    "print(np.shape(ws.sensor_pos.value))  # should be alt, SClat, SClon for each of nscans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn off jacobian calc for primary yCalc, keep clear-sky (non-scattering) setup\n",
    "ws.jacobianOff()\n",
    "ws.cloudboxOff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the surface agenda for ARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define surface agenda (need transmittance to run FASTEM):\n",
    "varnam = [\"Optical depth\"]\n",
    "ws.Copy(ws.iy_aux_vars, varnam)\n",
    "ws.VectorCreate( \"transmittance\" )\n",
    "#ws.transmittance = np.ones( ws.f_grid.value.shape ) * 0.0  #  FOR TESTING ONLY!!!!!\n",
    "\n",
    "@arts_agenda\n",
    "def iy_surface_agendaPY(ws):\n",
    "    ws.specular_losCalc()\n",
    "    # if wanting to test transmittance values, comment out next 3 calls and set outside agenda...\n",
    "    #  if ppathCalc is called and uses assumed inputs there are problems, so specify all!\n",
    "    ws.ppathCalc(ws.ppath_agenda, ws.ppath_lmax, \n",
    "       ws.ppath_lraytrace, ws.atmgeom_checked, ws.t_field, ws.z_field, \n",
    "       ws.vmr_field, ws.f_grid, ws.cloudbox_on, ws.cloudbox_checked, \n",
    "       ws.ppath_inside_cloudbox_do, ws.rtp_pos, ws.specular_los, ws.rte_pos2 )\n",
    "    ws.iyEmissionStandard()\n",
    "    ws.transmittanceFromIy_aux(transmittance=ws.transmittance)\n",
    "    ws.SurfaceFastem( transmittance = ws.transmittance, fastem_version=6 ) \n",
    "    ws.iySurfaceRtpropCalc()\n",
    "    \n",
    "ws.Copy(ws.iy_surface_agenda, iy_surface_agendaPY) # copy python-defined agenda to ARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform checks and set up mblock grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform some checks:\n",
    "ws.abs_xsec_agenda_checkedCalc()\n",
    "ws.propmat_clearsky_agenda_checkedCalc()\n",
    "ws.atmfields_checkedCalc( bad_partition_functions_ok = 1 )\n",
    "ws.atmgeom_checkedCalc()\n",
    "ws.cloudbox_checkedCalc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create vector mblock_reference_los and matrix mblock_target_los (to be used by DiffZaAa)\n",
    "mblock_reference_los = np.array(los0)\n",
    "\n",
    "# want to have all individual angle pairs distinct... so repeat one serially and one n times\n",
    "mblock_target_los = np.array(np.transpose( np.stack( \n",
    "                    [ np.repeat(za_grid, aa_grid.size) , \n",
    "                      np.tile(  aa_grid, za_grid.size) ]) ) , order = \"C\")\n",
    "## need to specify 'C' order or else stride is wrong and array values are read wrong by ARTS!\n",
    "\n",
    "# take the differences of zenith and azimuth angles to convert to chosen angle space\n",
    "ws.DiffZaAa(ws.mblock_dlos_grid, mblock_reference_los, mblock_target_los)\n",
    "\n",
    "#print(np.shape(ws.mblock_dlos_grid.value))\n",
    "#print(np.diff(ws.mblock_dlos_grid.value[0:10,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Calculate the sensor response function from the antenna response\n",
    "ws.VectorCreate(\"antenna_reference_los\")\n",
    "ws.MatrixCreate(\"antenna_target_los\")\n",
    "ws.antenna_reference_los = np.array(los0)\n",
    "ws.antenna_target_los = np.array(bsights, order=\"C\")\n",
    "ws.DiffZaAa(ws.antenna_dlos, ws.antenna_reference_los, ws.antenna_target_los)\n",
    "\n",
    "# define sensor... done above, but apply gf4 typhon/arts object to antenna_response:\n",
    "ws.sensor_norm = 1\n",
    "ws.antenna_dim = 2\n",
    "ws.antenna_response = gf4 \n",
    "\n",
    "ws.sensor_responseInit()\n",
    "\n",
    "## flipping from first two stokes components (I/Q) to V/H-pol Tb here... \n",
    "ws.instrument_pol = [5,6] # indices for V,H (1,2 are I,Q)\n",
    "\n",
    "ws.sensor_responsePolarisation()\n",
    "ws.sensor_responseAntenna()\n",
    "ws.sensor_checkedCalc()\n",
    "\n",
    "\n",
    "# should this get called? not yet -- will give bsights from designated positions, draw a line \n",
    "#ws.sensor_losGeometricFromSensorPosToOtherPositions(ws.target_pos=np.array([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first, set up the retrieval grid, which is independent from fwd model and\n",
    "#  independent from the background (lat/lon/alt) grid if chosen\n",
    "\n",
    "utx, uty = 0.8, 0.8      # grid defined radially out from center, so these are half-widths\n",
    "r_lims = [lat0-uty, lat0+uty, lon0-utx, lon0+utx] # lat/lon limits  (box in middle of overall box)\n",
    "r_resa  = resa*1.0         # tied to a factor of lat/lon grid resolution,\n",
    "r_reso  = reso*1.0         #  but not (necessarily) exact same points as lat/lon grid \n",
    "\n",
    "r_lon  = np.arange(r_lims[2], r_lims[3]+r_reso, r_reso)\n",
    "r_lat  = np.arange(r_lims[0], r_lims[1]+r_resa, r_resa)\n",
    "\n",
    "#print('retrieval grid resolution: ', r_resa, r_reso)\n",
    "print('retrieval lon grid: ', r_lon)\n",
    "print('retrieval lat grid: ', r_lat)\n",
    "\n",
    "# create grids of retrieval lats/lons for calculations of distance and plotting below\n",
    "rx, ry = np.meshgrid(r_lon, r_lat, indexing='ij')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set prior and covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# set Xa (formally set in ARTS below)\n",
    "xa = np.zeros([nrvar, r_lon.size, r_lat.size]) # a priori matrix, should be similar to s_data\n",
    "print('Xa shape: ', xa.shape)\n",
    "xa[0,:,:] = tmean #+ 0.07  # set constant, or maybe some perturbation\n",
    "if nrvar>1:\n",
    "    xa[1,:,:] = np.mean(s_data[1,:,:])  # if adding wind speed\n",
    "\n",
    "# then observation errors (first just sensor noise)\n",
    "se_cov = np.diag(np.tile(np.repeat(nedt**2,2), npix*nscans ))  # if nedt constant for all chans\n",
    "print('Se cov shape: ', se_cov.shape) # square matrix of side length npix*nscans*nf*2\n",
    "\n",
    "# calculate distances between points on retrieval grid for distance-dependent correlations in sx_cov\n",
    "dists = squareform( pdist( np.array([ry.flatten(), rx.flatten()]).transpose() ))\n",
    "\n",
    "# can try making decorrelation lengths gradient-dependent (wait for a non-flat prior for testing!):\n",
    "#rygrad, rxgrad = np.gradient(xa[:,:,0])\n",
    "#pseudo_grad = 0.5 * (np.abs(rygrad)*r_resa + np.abs(rxgrad)*r_reso)   # should be array of xa size\n",
    "## abs values, add gradients in x and y by retr grid resolution\n",
    "#deco_grid = np.ones_like(dists)*deco\n",
    "#deco_grid[pseudo_grad > 0] *= np.exp(-1.0*pseudo_grad[pseudo_grad>0])  \n",
    "## ad hoc, weight decorr length to be shorter if there's a non-zero gradient in apriori \n",
    "\n",
    "\n",
    "# then a priori state errors (Sa or Sx)\n",
    "len_rgrid = r_lon.size*r_lat.size # total number of points in retrieval grid\n",
    "nsx = np.sum(range(nrvar+1)) # 'triangle number' determines # of matrix blocks that define sx_cov\n",
    "\n",
    "# blocks of larger, sparse Sa unless the off-off diagonal elements are filled in (SST/wsp covariances)\n",
    "sx_cov = np.zeros([len_rgrid,len_rgrid, nsx])\n",
    "\n",
    "# common for both:\n",
    "sx_cov[:,:,0] = np.diag( np.tile(sx_sst**2, len_rgrid)) \n",
    "corrs_sst = np.exp(-dists/deco_sst) \n",
    "for x in range(len_rgrid):\n",
    "    for y in range(len_rgrid):\n",
    "        sx_cov[x,y,0]  = sx_sst * sx_sst * corrs_sst[x,y]  # i.e.  cov = r * sig * sig\n",
    "    \n",
    "if nrvar==2:  # wind and cross-correlations\n",
    "    corrs_wsp = np.exp(-dists/deco_wsp)\n",
    "    sx_cov[:,:,1] = np.diag( np.tile(sx_wsp**2, len_rgrid)) \n",
    "    for x in range(len_rgrid):\n",
    "        for y in range(len_rgrid):\n",
    "            sx_cov[x,y,1]  = sx_wsp * sx_wsp * corrs_wsp[x,y]  # if sx_wsp is function of loc, include here\n",
    "            # cross correlations! (w/ spatial corr from 1):\n",
    "            sx_cov[x,y,2]  = sx_wsp * sx_sst * corrs_wsp[x,y] * xcorr \n",
    "\n",
    "\n",
    "for rv in range(nsx):   # can plot each a priori covariance matrix block:\n",
    "    plt.matshow(sx_cov[:,:,rv])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize retrieval setup in ARTS, run yCalc for syn. observation vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formally set covarainces and prior for ARTS OEM:\n",
    "\n",
    "ws.retrievalDefInit()  # initialize then add variables\n",
    "\n",
    "for r in range(nrvar):\n",
    "    ws.retrievalAddSurfaceQuantity( g1=r_lat, g2=r_lon, quantity=s_names[r])\n",
    "    ws.covmat_sxAddBlock(block = np.copy(np.squeeze(sx_cov[:,:,r])), i=r, j=r )  # for 'diagonal' blocks\n",
    "    \n",
    "# for cross-correlation blocks (if more than 2 vars, revisit this):\n",
    "if nrvar>1:\n",
    "    ws.covmat_sxAddBlock(block = np.squeeze(sx_cov[:,:,2]), i=0, j=r )\n",
    "\n",
    "ws.Copy(ws.xa, xa.flatten())  # copy python xa to arts xa (everything collapsed to 1D)\n",
    "#ws.Print(ws.xa, 0) # Print Xa to terminal\n",
    "\n",
    "ws.covmat_seSet(se_cov)\n",
    "\n",
    "ws.retrievalDefClose()\n",
    "\n",
    "# first forward model call, to calculate synthetic observation vector\n",
    "ws.yCalc() \n",
    "initial_tb = np.copy(ws.y.value)  # copy into python memory\n",
    "#print(np.shape(initial_tb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the iteration agenda for ARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 'clip' limits for retrieved variables\n",
    "\n",
    "@arts_agenda\n",
    "def inversion_iterate_agenda(ws):\n",
    "    ws.Ignore(ws.inversion_iteration_counter) # on simon's advice\n",
    "    ws.xClip(ijq = 0, limit_low = 272.0, limit_high = 310.0)  #limits retrieval range of SST\n",
    "    if nrvar>1: ws.xClip(ijq = 1, limit_low = 0.5, limit_high = 20.0)  # limits wind_speed range\n",
    "    ws.x2artsAtmAndSurf()  # map x to ARTS's variables\n",
    "    \n",
    "    ws.yCalc() \n",
    "\n",
    "    ws.Copy(ws.y_baseline, np.zeros([initial_tb.size])) # kinda silly but necessary?\n",
    "    ws.VectorAddVector( ws.yf, ws.y, ws.y_baseline )  # add baseline term (need to create ws.yf)\n",
    "    \n",
    "    # this takes care of some fixes needed to get the jacobian right for iterative solutions:\n",
    "    ws.jacobianAdjustAndTransform()\n",
    "    \n",
    "\n",
    "ws.Copy(ws.inversion_iterate_agenda, inversion_iterate_agenda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturb synthetic measurement, call OEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before calling OEM, modify (synthetic) observation vector by adding noise and/or bias offsets:\n",
    "\n",
    "# add random gaussian noise to y -- mean, std, # of random samples\n",
    "for c in range(nf):   #for chan-specific sensor noise -- assumes V/H-pol have same noise \n",
    "    initial_tb[2*c::nf*2]   += np.random.normal(0, nedt[c], initial_tb[::nf*2].size)   #bias=0,\n",
    "    initial_tb[2*c+1::nf*2] += np.random.normal(0, nedt[c], initial_tb[::nf*2].size)   #gaussian noise\n",
    "ws.Copy(ws.y, initial_tb)  # copy to the ARTS y vector prior to running OEM\n",
    "\n",
    "if np.diagonal(se_cov).size != initial_tb.size:\n",
    "    print('Se covariance matrix and Tb vector not same size!')\n",
    "    aaaaandstop\n",
    "\n",
    "print('last gasp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call OEM:\n",
    "#  note: LM is better for non-linear problems and sfc retrieval is very linear, so stick with GN/GN_CG\n",
    "ws.OEM(method=\"gn\", #lm\",  # in one test LM took ~3x longer than GN!!\n",
    "    max_iter=12,\n",
    "    display_progress=1,\n",
    "    max_start_cost=1e5,\n",
    "    lm_ga_settings=np.array([10.0,2.0,3.0,10000000000.0,1.0,1.0])) # only applicable if method='lm'\n",
    "\n",
    "ws.Print(ws.oem_errors, 0)  # print any errors to terminal\n",
    "\n",
    "# calculate averaging kernel (A matrix) and a posteriori \n",
    "ws.avkCalc()\n",
    "Amat = ws.avk.value\n",
    "# A matrix should be [x,x] where x is each retrieved var. Trace(A) yields DOF for signal.\n",
    "print('Trace of A Matrix: ', np.trace(Amat))\n",
    "print('(out of possible points and vars:',np.diagonal(Amat).size)\n",
    "print('  ...so DOF per retr grid point: ',np.trace(Amat)/np.diagonal(Amat).size)\n",
    "#print(np.diagonal(Amat))\n",
    "ws.covmat_soCalc()\n",
    "post = ws.covmat_so.value  # posterior error covariance matrix\n",
    "# should be [x,x], providing uncertainties of each retrieved var, where x=npix*nscans*nretvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map variables back to scan/pixel arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# yf is y_fit whereas y itself is the measurement vector here, so obs-sim is y-yf:\n",
    "ydif = np.copy( ws.y.value - ws.yf.value )\n",
    "ysim = np.copy( ws.yf.value)\n",
    "\n",
    "xretr = np.copy( ws.x.value ).reshape(nrvar, r_lon.size, r_lat.size)  # are these right in lat vs. lon reconstruction?\n",
    "Amat_re = np.copy( np.diagonal(Amat) ).reshape(nrvar, r_lon.size, r_lat.size)\n",
    "xdif = xretr - xa   # save difference of apriori vs retrieved\n",
    "\n",
    "geo = np.copy(ws.y_geo.value) #columns are altitude, lat, lon, zenith, azimuth of measurement geoposition\n",
    "print(np.shape(ws.y.value))\n",
    "# y vector should be of size [nf*2 (stokes dim 2) ] * npix*nscans  \n",
    "\n",
    "# vars coming out of yCalc should be in order, nch then npix then nscans, i.e.:\n",
    "## with nch=6 first 6 values are from pix1,sc1 then next 6 are pix2,sc1 and so on -- be careful!\n",
    "# can compare geo[:,3] (az) to EIA from L1R (later), just 180-az to get EIA\n",
    "\n",
    "\n",
    "# convert vectors back to scan/pixel matrices:\n",
    "sim_tb = np.zeros([nscans,npix,nf*2])\n",
    "dif_tb = np.zeros([nscans,npix,nf*2]) \n",
    "arts_pos = np.zeros([nscans,npix,4]) \n",
    "print(nf*2,npix,nscans)\n",
    "\n",
    "for c in range(nf*2):\n",
    "    sim_tb[:,:,c] = ysim[c::nf*2].reshape(nscans,npix)\n",
    "    dif_tb[:,:,c] = ydif[c::nf*2].reshape(nscans,npix)\n",
    "    for bb in range(4):\n",
    "        arts_pos[:,:,bb] = geo[c::nf*2,bb+1].reshape(nscans,npix) \n",
    "        ## arts obs points -- can differ slightly from prescribed, defined where max response is registered!\n",
    "        \n",
    "    \n",
    "print(np.shape(dif_tb))\n",
    "plt.imshow(dif_tb[:,:,0]) # 6V obs-sim\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(dif_tb[:,:,nf*2-1]) # 10H obs-sim\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting representative ellipses:\n",
    "from matplotlib.patches import Ellipse\n",
    "from cartopy import crs as ccrs\n",
    "from cmocean import cm as cmo\n",
    "# first define widths/heights of ellipses that represent HPBW FOV for each freq:\n",
    "widths  = np.array([35., 35., 24., 14., 15.,  7., 3.])[fsub] # in km across\n",
    "heights = np.array([62., 62., 42., 22., 26., 12., 5.])[fsub] #  per freq, 6/7/10/18/23/36/89\n",
    "if los0[0] != 132.5:\n",
    "    print('ELLIPSE SIZES INCORRECT DUE TO non-AMSR sensor angle')\n",
    "\n",
    "# convert (roughly!) to deg near equator... okay as first approximation for plotting...\n",
    "#  if plotting near poles then will need to be more rigorous.\n",
    "widths *= 360.0/40075. / np.abs(np.cos(np.mean(ry))) # scale widths by cos(lat)\n",
    "print(widths)\n",
    "heights*= 360.0/40075. # deg/circumference\n",
    "\n",
    "\n",
    "#print('VERIFY THAT THESE ARE WHOLLY EVEN ON THE GRID!')\n",
    "#print(np.diff(arts_pos[0,:,1]))\n",
    "#print(np.diff(arts_pos[:,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ks = [0,nf*2-1] # channel indices to plot\n",
    "\n",
    "for k in ks:\n",
    "    # provide x,y coords, width, height, angle of each FOV-- make an ellipse\n",
    "    ells = [Ellipse(xy=[arts_pos[i,j,1],arts_pos[i,j,0]], width=widths[int(k/2)], height=heights[int(k/2)], \n",
    "                    angle=-arts_pos[i,j,3], linewidth=0.2, alpha=0.7, fill=False) \n",
    "            for i in range(nscans) for j in range(npix)]# for k in [0]] #range(nf*2)]\n",
    "\n",
    "    projj = ccrs.PlateCarree()\n",
    "    inn =  0.6\n",
    "\n",
    "    fig = plt.figure(figsize=[18,10])\n",
    "    ax = plt.subplot(111, projection=projj )\n",
    "\n",
    "    #reg = [np.min(arts_pos[:,:,0])+inn, np.max(arts_pos[:,:,0])-inn,\n",
    "    #       np.min(arts_pos[:,:,1])+inn, np.max(arts_pos[:,:,1])-inn]\n",
    "    reg = [np.min(la_grid)+inn, np.max(la_grid)-inn,\n",
    "           np.min(lo_grid)+inn, np.max(lo_grid)-inn]\n",
    "    ax.set_xlim(left=reg[2], right=reg[3])\n",
    "    ax.set_ylim(bottom=reg[0], top=reg[1])\n",
    "    nx,ny = 5,5\n",
    "    ax.set_xticks( np.linspace(reg[2],reg[3], nx) )\n",
    "    ax.set_yticks( np.linspace(reg[0],reg[1], ny) )\n",
    "\n",
    "    #sdif = np.max(np.abs(tmean-s_data[0,:,:])) # set colorbar limits by largest abs deviation from tmean?\n",
    "    thenorm1=plt.Normalize(vmin = np.min(s_data[0,:,:])-1, vmax = np.max(s_data[0,:,:])+1)\n",
    "    ceem1=cmo.dense\n",
    "    #bb= ax.pcolormesh(lon_grid,lat_grid,s_data[0,:,:],norm=thenorm1, # so SST\n",
    "    bb= ax.contourf(ws.lon_grid,ws.lat_grid,s_data[0,:,:], 10, norm=thenorm1, # so SST\n",
    "                 transform=projj, cmap=ceem1)\n",
    "\n",
    "    for e in ells[:]: \n",
    "        ax.add_artist(e)\n",
    "\n",
    "    vmin, vmax = -np.max(np.abs(dif_tb[:,:,k])), np.max(np.abs(dif_tb[:,:,k]))\n",
    "    thenorm = plt.Normalize(vmin=vmin,vmax=vmax)\n",
    "    ceem = cmo.balance #thermal\n",
    "    ax.scatter( arts_pos[:,:,1], arts_pos[:,:,0], s=100.0, c=dif_tb[:,:,k], marker='o', \\\n",
    "                transform=projj, alpha=0.8, cmap=ceem, norm=thenorm);\n",
    "    sm = plt.cm.ScalarMappable(cmap=ceem, norm=thenorm)\n",
    "    sm._A = []\n",
    "    cb = plt.colorbar(sm,ax=ax)\n",
    "    cb.set_label(ch_str[k]+' TB [K] (obs-sim)')\n",
    "    sm1 = plt.cm.ScalarMappable(cmap=ceem1, norm=thenorm1)\n",
    "    sm1._A = []\n",
    "    cb1 = plt.colorbar(sm1,ax=ax)\n",
    "    cb1.set_label('SST [K]')\n",
    "\n",
    "    if sv: plt.savefig('img2/retr-y_syn__'+ch_str[k]+'_'+pv+'.png',bbox_inches='tight',dpi=350)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# and for the state vector, retrieved vs. a priori laid over background (truth):\n",
    "\n",
    "for rv in range(nrvar): \n",
    "    if rv==0: tit='SST [K]'\n",
    "    if rv==1: tit='Wind speed [$m s^{-1}$]'\n",
    "    fig2 = plt.figure(figsize=[18,10])\n",
    "    ax2 = plt.subplot(111, projection=projj )\n",
    "\n",
    "    ax2.set_xlim(left=reg[2], right=reg[3])\n",
    "    ax2.set_ylim(bottom=reg[0], top=reg[1])\n",
    "    ax2.set_xticks( np.linspace(reg[2],reg[3], nx) )\n",
    "    ax2.set_yticks( np.linspace(reg[0],reg[1], ny) )\n",
    "    ax2.set_ylabel( 'Latitude [$^o N$]' )\n",
    "    ax2.set_xlabel( 'Longitude [$^o E$]' )\n",
    "\n",
    "    thenorm1=plt.Normalize(vmin = np.min(s_data[rv,:,:])-1, vmax = np.max(s_data[rv,:,:])+1)\n",
    "    #bb= ax2.pcolormesh(lon_grid,lat_grid,s_data[0,:,:],norm=thenorm1, \n",
    "    bb= ax2.contourf(ws.lon_grid,ws.lat_grid,s_data[rv,:,:], 10, norm=thenorm1, \n",
    "                 transform=projj, cmap=ceem1)\n",
    "\n",
    "    #for e in ells[:]: \n",
    "    #    ax2.add_artist(e)\n",
    "\n",
    "    vmin, vmax = -np.max(np.abs(xdif[rv,:,:])), np.max(np.abs(xdif[rv,:,:]))\n",
    "    thenorm = plt.Normalize(vmin=vmin,vmax=vmax)\n",
    "    ceem = cmo.balance #thermal\n",
    "    # rx,ry defined earlier via meshgrid, i.e. retrieval lon and lat grids respectively\n",
    "    ax2.scatter( rx, ry, s=200.0, c=xdif[rv,:,:], marker='o', \\\n",
    "                transform=projj, alpha=0.8, cmap=ceem, norm=thenorm);\n",
    "    sm = plt.cm.ScalarMappable(cmap=ceem, norm=thenorm)\n",
    "    sm._A = []\n",
    "    cb = plt.colorbar(sm,ax=ax2)\n",
    "    cb.set_label(tit+' (retr - prior)')\n",
    "    sm1 = plt.cm.ScalarMappable(cmap=ceem1, norm=thenorm1)\n",
    "    sm1._A = []\n",
    "    cb1 = plt.colorbar(sm1,ax=ax2)\n",
    "    cb1.set_label(tit)\n",
    "\n",
    "    if sv: plt.savefig('img2/retr-x_syn_rv'+str(rv)+'_'+pv+'.png',bbox_inches='tight',dpi=350)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stapppp # who cares (for now)\n",
    "\n",
    "# and for the DOF, take elements from A matrix diagonal:\n",
    "fig3 = plt.figure(figsize=[15,10])\n",
    "ax3 = plt.subplot(111, projection=projj )\n",
    "\n",
    "ax3.set_xlim(left=reg[2], right=reg[3])\n",
    "ax3.set_ylim(bottom=reg[0], top=reg[1])\n",
    "ax3.set_xticks( np.linspace(reg[2],reg[3], nx) )\n",
    "ax3.set_yticks( np.linspace(reg[0],reg[1], ny) )\n",
    "\n",
    "bb= ax3.pcolormesh(lon_grid,lat_grid,s_data[0,:,:],norm=thenorm1, # so SST\n",
    "             transform=projj, cmap=ceem1)\n",
    "\n",
    "#for e in ells[:]: \n",
    "#    ax3.add_artist(e)\n",
    "    \n",
    "vmin, vmax = 0, 1 #-np.max(np.abs(xdif[:,:])), np.max(np.abs(xdif[:,:]))\n",
    "thenorm = plt.Normalize(vmin=vmin,vmax=vmax)\n",
    "ceem = cmo.solar\n",
    "# rx,ry defined earlier via meshgrid\n",
    "ax3.scatter( rx, ry, s=200.0, c=Amat_re[0,:,:], marker='o',    # pick which Amat vars?\n",
    "            transform=projj, alpha=0.8, cmap=ceem, norm=thenorm);\n",
    "sm = plt.cm.ScalarMappable(cmap=ceem, norm=thenorm)\n",
    "sm._A = []\n",
    "cb = plt.colorbar(sm,ax=ax3)\n",
    "cb.set_label('DOFS')\n",
    "sm1 = plt.cm.ScalarMappable(cmap=ceem1, norm=thenorm1)\n",
    "sm1._A = []\n",
    "cb1 = plt.colorbar(sm1,ax=ax3)\n",
    "cb1.set_label('SST [K]')\n",
    "\n",
    "#plt.savefig('img2/retr-A_syn_'+pv+'.png',bbox_inches='tight',dpi=350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to run fastem alone for testing, outputting emissivities and expected TBs with no atmosphere:\n",
    "#ws.MatrixCreate(\"emissivity\")\n",
    "#ws.MatrixCreate(\"reflectivity\")\n",
    "#ws.reflectivity = np.zeros([f_grid.size,4])\n",
    "#ws.emissivity   = np.zeros([f_grid.size,4])\n",
    "##ws.surface_skin_t = tmean #s_data[0,:,:]\n",
    "##ws.transmittance\n",
    "#ws.transmittance = np.ones( ws.f_grid.value.shape ) * 0.7\n",
    "#yep = 34\n",
    "#emii = np.zeros([yep,nf,2])\n",
    "#tbout= np.zeros([yep,nf,2])\n",
    "#tmean=270.0\n",
    "#for y in range(yep):\n",
    "#    ws.surface_skin_t = tmean+float(y) #s_data[0,:,:]\n",
    "#    ws.FastemStandAlone(surface_skin_t= tmean+y, f_grid=ws.f_grid,\n",
    "#                        emissivity=ws.emissivity, reflectivity=ws.reflectivity,\n",
    "#                        za=125.0, wind_speed=wsp, rel_aa=0.0, transmittance=ws.transmittance.value)\n",
    "#    emii[y,:,:] = ws.emissivity.value[:,0:2]\n",
    "#    tbout[y,:,:]= ws.emissivity.value[:,0:2]*tmean+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### soooo, emissivity varies quite little with temperature for 6V (0.557->.553->.56),\n",
    "#     declines slightly at 18 (.64->.58), then bigtime at 89 (.84->.74)\n",
    "##print(ws.emissivity.value)\n",
    "#print(emii[:,0,0]) # 6\n",
    "##print(emii[:,3,0]) # 18\n",
    "##print(emii[:,5,0]) # 36\n",
    "#print(emii[:,-1,0]) # 89\n",
    "#\n",
    "#print(tbout[:,1,0])\n",
    "##print(tbout[:,2,0])\n",
    "#print(tbout[:,-1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
